{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53edd431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from scipy.stats import gaussian_kde, norm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import scipy.stats as st\n",
    "from scipy.stats import gaussian_kde\n",
    "from typing import Sequence, Tuple, Dict, Optional, Callable\n",
    "\n",
    "print(\"Running with PyMC version:\", pm.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda2add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths \n",
    "# here we have paths to folders where the logging histories of individual training runs are stored.\n",
    "\n",
    "# for Unet and Swin Transformer models trained on MACS, we have two sets of logs:\n",
    "unet_macs_loss_dir = r'N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\logs\\unet_ae_samples'\n",
    "swin_macs_loss_dir = r'N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\logs\\swin_ae_samples'\n",
    "\n",
    "# for Unet and Swin Transformer models trained on PS, we have two sets of logs:\n",
    "unet_ps_loss_dir = r'N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\logs\\unet_ps_samples'\n",
    "swin_ps_loss_dir = r'N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\logs\\swin_ps_samples'\n",
    "\n",
    "\n",
    "# metrics to analyse\n",
    "metrics = ['loss', 'accuracy', 'specificity', 'sensitivity', 'IoU', 'f1_score', 'Hausdorff_distance']\n",
    "\n",
    "# which metrics are \"higher is better\"\n",
    "maximize_metrics = {'accuracy', 'specificity', 'sensitivity', 'IoU', 'f1_score'}  # include accuracy too\n",
    "\n",
    "# metrics that are bounded in (0,1) and should be modelled on logit scale\n",
    "bounded_01_metrics = {'accuracy', 'specificity', 'sensitivity', 'IoU', 'f1_score'}\n",
    "\n",
    "# priors for factorial effects (on transformed scale)\n",
    "# - logit scale: Normal(0, 0.5) is a reasonable weakly-informative prior\n",
    "# - log scale  : Normal(0, 0.2) corresponds to ~ +/- 22% multiplicative change (1 SD)\n",
    "prior_sd_logit = 0.5\n",
    "prior_sd_log = 0.2\n",
    "\n",
    "# sampling config\n",
    "DRAWS = 2000\n",
    "TUNE = 2000\n",
    "CHAINS = 4\n",
    "TARGET_ACCEPT = 0.9\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Savage–Dickey KDE bandwidth (can be adjusted)\n",
    "SD_KDE_BW = 0.3\n",
    "\n",
    "# overall ranking weights across metrics (explicit utility)\n",
    "# leave as None for equal weights across metrics\n",
    "metric_weights: Optional[Dict[str, float]] = None\n",
    "\n",
    "# output dirs (optional; keep yours)\n",
    "unet_macs_output_dir = unet_macs_loss_dir\n",
    "swin_macs_output_dir = swin_macs_loss_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c92b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metrics_as_array(directory, metrics):\n",
    "    \"\"\"\n",
    "    Reads CSV files from a directory and extracts specified metrics into a 3D numpy array.\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing CSV files.\n",
    "        metrics (list): List of metric names to extract from the CSV files.\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - data_array (np.ndarray): A 3D numpy array of shape (num_files, num_epochs, num_metrics).\n",
    "            - lookup (dict): A dictionary mapping metric names to their indices in the data array.\n",
    "            - files (list): List of file names processed.\n",
    "    \"\"\"\n",
    "\n",
    "    files = sorted([f for f in os.listdir(directory) if f.endswith('.csv')])\n",
    "    data_list = []\n",
    "    \n",
    "    metric_names = []\n",
    "    for metric in metrics:\n",
    "        metric_names.append(metric)\n",
    "        metric_names.append('val_' + metric)\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        file_data = []\n",
    "\n",
    "        for name in metric_names:\n",
    "            if name in df.columns:\n",
    "                file_data.append(df[name].values)\n",
    "            else:\n",
    "                # Fill with NaNs if column is missing\n",
    "                file_data.append(np.full(len(df), np.nan))\n",
    "        \n",
    "        # shape (epochs, metric_columns)\n",
    "        file_data = np.stack(file_data, axis=1)\n",
    "        data_list.append(file_data)\n",
    "\n",
    "    # shape (files, epochs, metric_columns)\n",
    "    data_array = np.stack(data_list, axis=0)\n",
    "\n",
    "    # lookup metric name -> column index\n",
    "    lookup = {name: idx for idx, name in enumerate(metric_names)}\n",
    "\n",
    "    return data_array, lookup, files\n",
    "\n",
    "\n",
    "# import arrays for each condition\n",
    "unet_macs, unet_macs_metric_lookup, unet_macs_file_names = read_metrics_as_array(unet_macs_loss_dir, metrics)\n",
    "swin_macs, swin_macs_metric_lookup, swin_macs_file_names = read_metrics_as_array(swin_macs_loss_dir, metrics)\n",
    "unet_ps, unet_ps_metric_lookup, unet_ps_file_names = read_metrics_as_array(unet_ps_loss_dir, metrics)\n",
    "swin_ps, swin_ps_metric_lookup, swin_ps_file_names = read_metrics_as_array(swin_ps_loss_dir, metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac848cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scientific choice:\n",
    "# We represent each training run by the metric value at the epoch where validation loss is minimal\n",
    "# (i.e., \"checkpoint selected by min val_loss\"). This mirrors common early-stopping/model-selection practice.\n",
    "\n",
    "def get_best_metric_at_min_val_loss(data_array, metric_lookup, metric):\n",
    "    \"\"\"\n",
    "    For each run/file:\n",
    "      - pick epoch where val_loss is minimal (fallback to loss if val_loss not present)\n",
    "      - return the metric value at that epoch\n",
    "    Returns:\n",
    "        np.ndarray of shape (num_runs,)\n",
    "    \"\"\"\n",
    "\n",
    "    # choose selection criterion\n",
    "    if \"val_loss\" in metric_lookup:\n",
    "        sel_loss_name = \"val_loss\"\n",
    "    elif \"loss\" in metric_lookup:\n",
    "        sel_loss_name = \"loss\"\n",
    "    else:\n",
    "        raise ValueError(\"Neither 'val_loss' nor 'loss' found in the metric lookup.\")\n",
    "\n",
    "    best_values = []\n",
    "    for i in range(data_array.shape[0]):\n",
    "        losses = data_array[i, :, metric_lookup[sel_loss_name]]\n",
    "        values = data_array[i, :, metric_lookup[metric]]\n",
    "        best_epoch = np.nanargmin(losses)\n",
    "        best_values.append(values[best_epoch])\n",
    "\n",
    "    return np.array(best_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3607dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We model bounded metrics (IoU, F1, sensitivity, ...) on logit scale.\n",
    "# We model positive metrics (loss, Hausdorff, ...) on log scale.\n",
    "# This avoids Normal likelihood pathologies at boundaries and ensures positivity.\n",
    "\n",
    "def base_metric_name(metric_col: str) -> str:\n",
    "    return metric_col[4:] if metric_col.startswith(\"val_\") else metric_col\n",
    "\n",
    "def is_bounded_01(metric_col: str) -> bool:\n",
    "    return base_metric_name(metric_col) in bounded_01_metrics\n",
    "\n",
    "def higher_is_better(metric_col: str) -> bool:\n",
    "    m = base_metric_name(metric_col)\n",
    "    return m in maximize_metrics\n",
    "\n",
    "def transform_y(y: np.ndarray, metric_col: str, eps: float = 1e-6) -> Tuple[np.ndarray, Callable[[np.ndarray], np.ndarray], str]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      y_transformed,\n",
    "      inverse_transform,\n",
    "      transform_name\n",
    "    \"\"\"\n",
    "    if is_bounded_01(metric_col):\n",
    "        y_clip = np.clip(y, eps, 1 - eps)\n",
    "        y_t = np.log(y_clip / (1 - y_clip))         # logit\n",
    "        inv = lambda z: 1 / (1 + np.exp(-z))\n",
    "        return y_t, inv, \"logit\"\n",
    "    else:\n",
    "        y_clip = np.clip(y, eps, None)\n",
    "        y_t = np.log(y_clip)                        # log\n",
    "        inv = lambda z: np.exp(z)\n",
    "        return y_t, inv, \"log\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38eae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_probabilities_from_draws(draws: np.ndarray, group_names, higher_better: bool) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    draws: (S, G) posterior draws of each group's performance (ORIGINAL scale preferred)\n",
    "    higher_better: True if larger = better, else smaller = better\n",
    "    \"\"\"\n",
    "    S, G = draws.shape\n",
    "    score = draws if higher_better else -draws\n",
    "\n",
    "    order = np.argsort(-score, axis=1)  # best..worst\n",
    "    ranks = np.empty_like(order)\n",
    "    for s in range(S):\n",
    "        ranks[s, order[s]] = np.arange(1, G + 1)\n",
    "\n",
    "    out = {\"group\": list(group_names)}\n",
    "    for k in range(1, G + 1):\n",
    "        out[f\"Pr(rank={k})\"] = [(ranks[:, j] == k).mean() for j in range(G)]\n",
    "    out[\"E[rank]\"] = [ranks[:, j].mean() for j in range(G)]\n",
    "    out[\"Pr(best)\"] = out[\"Pr(rank=1)\"]\n",
    "\n",
    "    return pd.DataFrame(out).sort_values(\"E[rank]\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef908f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BF10 = prior_density(Δ=0) / posterior_density(Δ=0)\n",
    "# Requires idata.prior draws for the effect parameter.\n",
    "\n",
    "def _density_at_zero(draws_1d: np.ndarray, bw=SD_KDE_BW) -> float:\n",
    "    draws_1d = np.asarray(draws_1d).ravel()\n",
    "    kde = gaussian_kde(draws_1d, bw_method=bw)\n",
    "    return float(kde.evaluate(0.0)[0])\n",
    "\n",
    "def savage_dickey_bf10(idata: az.InferenceData, var_name: str, bw=SD_KDE_BW) -> float:\n",
    "    \"\"\"\n",
    "    Savage–Dickey BF10 for H1: var != 0 vs H0: var = 0\n",
    "    BF10 = p(var=0 | prior) / p(var=0 | posterior)\n",
    "    \"\"\"\n",
    "    if not hasattr(idata, \"prior\"):\n",
    "        raise ValueError(\n",
    "            \"idata has no prior group. Ensure you ran pm.sample_prior_predictive(var_names=[...]) \"\n",
    "            \"and extended idata with it.\"\n",
    "        )\n",
    "\n",
    "    post = az.extract(idata, group=\"posterior\", var_names=[var_name]).to_numpy().ravel()\n",
    "    prior = az.extract(idata, group=\"prior\", var_names=[var_name]).to_numpy().ravel()\n",
    "\n",
    "    prior0 = _density_at_zero(prior, bw=bw)\n",
    "    post0  = _density_at_zero(post,  bw=bw)\n",
    "\n",
    "    return prior0 / post0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07c07ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_factorial_model_with_bf(\n",
    "    df: pd.DataFrame,\n",
    "    dataset_col: str,\n",
    "    arch_col: str,\n",
    "    metric_col: str,\n",
    "    draws=DRAWS,\n",
    "    tune=TUNE,\n",
    "    chains=CHAINS,\n",
    "    target_accept=TARGET_ACCEPT,\n",
    "    seed=RANDOM_SEED,\n",
    "    bw=SD_KDE_BW\n",
    "):\n",
    "    data = df[[dataset_col, arch_col, metric_col]].dropna().copy()\n",
    "\n",
    "    dcat = pd.Categorical(data[dataset_col])\n",
    "    acat = pd.Categorical(data[arch_col])\n",
    "\n",
    "    if len(dcat.categories) != 2 or len(acat.categories) != 2:\n",
    "        raise ValueError(\"Factorial model expects exactly 2 dataset levels and 2 architecture levels.\")\n",
    "\n",
    "    # 0/1 coding\n",
    "    d = (dcat.codes == 1).astype(int)     # dataset(1) - dataset(0)\n",
    "    a = (acat.codes == 1).astype(int)     # arch(1) - arch(0)\n",
    "    da = d * a\n",
    "\n",
    "    y_raw = data[metric_col].to_numpy()\n",
    "    y_t, inv, tname = transform_y(y_raw, metric_col)\n",
    "\n",
    "    # meaningful prior scale (on transformed scale)\n",
    "    effect_sd = prior_sd_logit if tname == \"logit\" else prior_sd_log\n",
    "\n",
    "    with pm.Model() as model:\n",
    "        d_data = pm.Data(\"d\", d)\n",
    "        a_data = pm.Data(\"a\", a)\n",
    "        da_data = pm.Data(\"da\", da)\n",
    "\n",
    "        intercept = pm.Normal(\"intercept\", 0.0, 1.5)\n",
    "\n",
    "        beta_dataset = pm.Normal(\"beta_dataset\", 0.0, effect_sd)\n",
    "        beta_arch = pm.Normal(\"beta_arch\", 0.0, effect_sd)\n",
    "        beta_interaction = pm.Normal(\"beta_interaction\", 0.0, effect_sd)\n",
    "\n",
    "        # robust likelihood\n",
    "        sigma = pm.HalfNormal(\"sigma\", 1.0)\n",
    "        nu = pm.Exponential(\"nu\", 1/30) + 1\n",
    "\n",
    "        mu = intercept + beta_dataset*d_data + beta_arch*a_data + beta_interaction*da_data\n",
    "\n",
    "        pm.StudentT(\"y\", nu=nu, mu=mu, sigma=sigma, observed=y_t)\n",
    "\n",
    "        idata = pm.sample(\n",
    "            draws=draws,\n",
    "            tune=tune,\n",
    "            chains=chains,\n",
    "            target_accept=target_accept,\n",
    "            random_seed=seed,\n",
    "            return_inferencedata=True\n",
    "        )\n",
    "\n",
    "        # prior draws for Savage–Dickey (must include the effect vars)\n",
    "        prior = pm.sample_prior_predictive(\n",
    "            var_names=[\"beta_dataset\", \"beta_arch\", \"beta_interaction\"],\n",
    "            random_seed=seed\n",
    "        )\n",
    "        idata.extend(prior)\n",
    "\n",
    "    # effect size + uncertainty + BF10 (on transformed scale)\n",
    "    summ = az.summary(idata, var_names=[\"beta_dataset\",\"beta_arch\",\"beta_interaction\"], hdi_prob=0.94).reset_index()\n",
    "    summ = summ.rename(columns={\"index\":\"param\"})\n",
    "\n",
    "    # directional probability\n",
    "    for v in [\"beta_dataset\",\"beta_arch\",\"beta_interaction\"]:\n",
    "        draws_v = az.extract(idata, group=\"posterior\", var_names=[v]).to_numpy().ravel()\n",
    "        summ.loc[summ[\"param\"] == v, \"Pr(>0)\"] = (draws_v > 0).mean()\n",
    "        summ.loc[summ[\"param\"] == v, \"BF10\"] = savage_dickey_bf10(idata, v, bw=bw)\n",
    "\n",
    "    # label effects with category names\n",
    "    d0, d1 = list(dcat.categories)\n",
    "    a0, a1 = list(acat.categories)\n",
    "    label_map = {\n",
    "        \"beta_dataset\": f\"dataset effect ({d1} - {d0})\",\n",
    "        \"beta_arch\": f\"architecture effect ({a1} - {a0})\",\n",
    "        \"beta_interaction\": \"interaction (difference-in-differences)\"\n",
    "    }\n",
    "    summ[\"effect\"] = summ[\"param\"].map(label_map)\n",
    "\n",
    "    report = summ.loc[:, [\"effect\",\"mean\",\"sd\",\"hdi_3%\",\"hdi_97%\",\"Pr(>0)\",\"BF10\"]].copy()\n",
    "    report[\"scale\"] = tname\n",
    "    report[\"prior\"] = f\"Normal(0, {effect_sd}) on {tname} scale\"\n",
    "\n",
    "    return idata, report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a01ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We model on transformed scale, but report group means on ORIGINAL scale.\n",
    "\n",
    "def fit_group_model_with_ranking(\n",
    "    df: pd.DataFrame,\n",
    "    group_col: str,\n",
    "    metric_col: str,\n",
    "    draws=DRAWS,\n",
    "    tune=TUNE,\n",
    "    chains=CHAINS,\n",
    "    target_accept=TARGET_ACCEPT,\n",
    "    seed=RANDOM_SEED\n",
    "):\n",
    "    data = df[[group_col, metric_col]].dropna().copy()\n",
    "    gcat = pd.Categorical(data[group_col])\n",
    "    g_idx = gcat.codes\n",
    "    groups = list(gcat.categories)\n",
    "    G = len(groups)\n",
    "\n",
    "    y_raw = data[metric_col].to_numpy()\n",
    "    y_t, inv, tname = transform_y(y_raw, metric_col)\n",
    "\n",
    "    coords = {\"group\": groups}\n",
    "\n",
    "    with pm.Model(coords=coords) as model:\n",
    "        g = pm.Data(\"g\", g_idx)\n",
    "        y_obs = pm.Data(\"y_obs\", y_t)\n",
    "\n",
    "        # hierarchical priors (transformed scale)\n",
    "        mu0 = pm.Normal(\"mu0\", 0.0, 1.5)\n",
    "        tau = pm.HalfNormal(\"tau\", 1.0)\n",
    "\n",
    "        mu = pm.Normal(\"mu\", mu0, tau, dims=\"group\")\n",
    "        sigma = pm.HalfNormal(\"sigma\", 1.0, dims=\"group\")\n",
    "\n",
    "        nu = pm.Exponential(\"nu\", 1/30) + 1\n",
    "\n",
    "        pm.StudentT(\"y\", nu=nu, mu=mu[g], sigma=sigma[g], observed=y_obs)\n",
    "\n",
    "        # deterministic group means on original scale\n",
    "        mu_orig = pm.Deterministic(\"mu_orig\", inv(mu), dims=\"group\")\n",
    "\n",
    "        idata = pm.sample(\n",
    "            draws=draws,\n",
    "            tune=tune,\n",
    "            chains=chains,\n",
    "            target_accept=target_accept,\n",
    "            random_seed=seed,\n",
    "            return_inferencedata=True\n",
    "        )\n",
    "\n",
    "    # posterior draws for ranking (original scale)\n",
    "    mu_draws = (\n",
    "        idata.posterior[\"mu_orig\"]\n",
    "        .stack(sample=(\"chain\",\"draw\"))\n",
    "        .transpose(\"sample\",\"group\")\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    hib = higher_is_better(metric_col)\n",
    "\n",
    "    rank_table = rank_probabilities_from_draws(mu_draws, groups, higher_better=hib)\n",
    "\n",
    "    # summary table (original scale)\n",
    "    summ = az.summary(idata, var_names=[\"mu_orig\"], hdi_prob=0.94).reset_index()\n",
    "    summ = summ.rename(columns={\"index\":\"param\"})\n",
    "    summ[\"group_idx\"] = summ[\"param\"].str.extract(r\"mu_orig\\[(\\d+)\\]\")[0].astype(int)\n",
    "    summ[\"group\"] = summ[\"group_idx\"].apply(lambda i: groups[i])\n",
    "\n",
    "    perf_table = (\n",
    "        summ.loc[:, [\"group\",\"mean\",\"sd\",\"hdi_3%\",\"hdi_97%\"]]\n",
    "        .merge(rank_table, on=\"group\", how=\"left\")\n",
    "        .sort_values(\"E[rank]\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return idata, perf_table, rank_table, mu_draws, tname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eff4190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a composite score:\n",
    "#  - for each metric, ensure \"higher is better\" by sign-flip if necessary\n",
    "#  - standardise per metric so scales don't dominate\n",
    "#  - weighted average across metrics (equal weights by default)\n",
    "#\n",
    "# Then compute posterior rank probabilities from composite draws.\n",
    "\n",
    "def overall_ranking_across_metrics(\n",
    "    mu_draws_by_metric: Dict[str, np.ndarray],\n",
    "    higher_is_better_by_metric: Dict[str, bool],\n",
    "    group_names: Sequence[str],\n",
    "    weights: Optional[Dict[str, float]] = None,\n",
    "):\n",
    "    metric_list = list(mu_draws_by_metric.keys())\n",
    "\n",
    "    if weights is None:\n",
    "        weights = {m: 1.0 for m in metric_list}\n",
    "    wsum = sum(weights.values())\n",
    "    weights = {m: weights[m] / wsum for m in metric_list}\n",
    "\n",
    "    # align draws\n",
    "    S = min(mu_draws_by_metric[m].shape[0] for m in metric_list)\n",
    "    G = len(group_names)\n",
    "\n",
    "    composite = np.zeros((S, G))\n",
    "    for m in metric_list:\n",
    "        mu = mu_draws_by_metric[m][:S, :]\n",
    "        score = mu if higher_is_better_by_metric[m] else -mu\n",
    "\n",
    "        # standardize to avoid scale dominance\n",
    "        sd = score.std() + 1e-12\n",
    "        z = score / sd\n",
    "\n",
    "        composite += weights[m] * z\n",
    "\n",
    "    overall_rank_table = rank_probabilities_from_draws(composite, group_names, higher_better=True)\n",
    "\n",
    "    return composite, overall_rank_table, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78a12d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For each metric:\n",
    "#  1) extract per-run \"best checkpoint\" values for 4 combinations\n",
    "#  2) build dataframe with factors (group/dataset/arch)\n",
    "#  3) plot distribution\n",
    "#  4) fit:\n",
    "#     - hierarchical group model (ranking)\n",
    "#     - factorial model (effects + Savage–Dickey BF10)\n",
    "#  5) store posterior draws for overall multi-metric ranking\n",
    "\n",
    "tabs = []\n",
    "tab_titles = []\n",
    "\n",
    "# store posterior group-mean draws per metric for overall ranking\n",
    "mu_draws_by_metric: Dict[str, np.ndarray] = {}\n",
    "hib_by_metric: Dict[str, bool] = {}\n",
    "group_order = [\"U-Net | Aerial\", \"Swin U-Net | Aerial\", \"U-Net | PS\", \"Swin U-Net | PS\"]  # consistent order\n",
    "\n",
    "for metric in metrics:\n",
    "    full_metric = 'val_' + metric if not metric.startswith('val_') else metric\n",
    "    out = widgets.Output()\n",
    "\n",
    "    with out:\n",
    "        clear_output(wait=True)\n",
    "        print(f'Processing: {full_metric} at point of best loss\\n')\n",
    "\n",
    "        # per-run values at best checkpoint\n",
    "        unet_macs_best = get_best_metric_at_min_val_loss(unet_macs, unet_macs_metric_lookup, full_metric)\n",
    "        swin_macs_best = get_best_metric_at_min_val_loss(swin_macs, swin_macs_metric_lookup, full_metric)\n",
    "        unet_ps_best   = get_best_metric_at_min_val_loss(unet_ps,   unet_ps_metric_lookup,   full_metric)\n",
    "        swin_ps_best   = get_best_metric_at_min_val_loss(swin_ps,   swin_ps_metric_lookup,   full_metric)\n",
    "\n",
    "        # long dataframe with factorial coding\n",
    "        data = pd.concat([\n",
    "            pd.DataFrame({full_metric: unet_macs_best, \"group\": \"U-Net | Aerial\",      \"dataset\": \"Aerial\", \"arch\": \"U-Net\"}),\n",
    "            pd.DataFrame({full_metric: swin_macs_best, \"group\": \"Swin U-Net | Aerial\", \"dataset\": \"Aerial\", \"arch\": \"Swin\"}),\n",
    "            pd.DataFrame({full_metric: unet_ps_best,   \"group\": \"U-Net | PS\",          \"dataset\": \"PS\",     \"arch\": \"U-Net\"}),\n",
    "            pd.DataFrame({full_metric: swin_ps_best,   \"group\": \"Swin U-Net | PS\",     \"dataset\": \"PS\",     \"arch\": \"Swin\"}),\n",
    "        ], ignore_index=True)\n",
    "\n",
    "        # enforce group order\n",
    "        data[\"group\"] = pd.Categorical(data[\"group\"], categories=group_order, ordered=True)\n",
    "\n",
    "        # visualize run-to-run distribution\n",
    "        sns.kdeplot(data=data, x=full_metric, hue=\"group\")\n",
    "        plt.title(f\"Distribution across runs: {full_metric}\")\n",
    "        plt.show()\n",
    "\n",
    "        # 1) hierarchical group model -> rank probabilities\n",
    "        idata_g, perf_table, rank_table, mu_draws, scale_name = fit_group_model_with_ranking(\n",
    "            data, group_col=\"group\", metric_col=full_metric\n",
    "        )\n",
    "\n",
    "        print(\"\\n--- Hierarchical group model (group means on original scale) ---\")\n",
    "        print(f\"Modelled on {scale_name} scale; reporting mu_orig on original scale.\\n\")\n",
    "        display(perf_table)\n",
    "\n",
    "        # store for overall ranking\n",
    "        mu_draws_by_metric[full_metric] = mu_draws\n",
    "        hib_by_metric[full_metric] = higher_is_better(full_metric)\n",
    "\n",
    "        # 2) factorial model -> effect size + uncertainty + BF10 (Savage–Dickey)\n",
    "        idata_f, factorial_report = fit_factorial_model_with_bf(\n",
    "            data, dataset_col=\"dataset\", arch_col=\"arch\", metric_col=full_metric\n",
    "        )\n",
    "\n",
    "        print(\"\\n--- Factorial effects (transformed scale) + Savage–Dickey BF10 ---\")\n",
    "        display(factorial_report)\n",
    "\n",
    "    tabs.append(out)\n",
    "    tab_titles.append(full_metric)\n",
    "\n",
    "tab_widget = widgets.Tab(children=tabs)\n",
    "for i, title in enumerate(tab_titles):\n",
    "    tab_widget.set_title(i, title)\n",
    "\n",
    "display(tab_widget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db4c343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This produces a single posterior ranking of the 4 combinations \"overall\".\n",
    "# IMPORTANT: \"overall\" depends on the utility you define (weights + standardization).\n",
    "# Default: equal weights.\n",
    "\n",
    "composite_draws, overall_rank_table, used_weights = overall_ranking_across_metrics(\n",
    "    mu_draws_by_metric=mu_draws_by_metric,\n",
    "    higher_is_better_by_metric=hib_by_metric,\n",
    "    group_names=group_order,\n",
    "    weights=metric_weights\n",
    ")\n",
    "\n",
    "print(\"\\n===============================\")\n",
    "print(\"OVERALL MULTI-METRIC RANKING\")\n",
    "print(\"===============================\\n\")\n",
    "\n",
    "display(overall_rank_table)\n",
    "\n",
    "print(\"\\nWeights used:\")\n",
    "print(used_weights)\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Pr(rank=1) is the posterior probability a combination is best overall under this utility.\")\n",
    "print(\"- E[rank] is the expected rank (lower = better).\")\n",
    "print(\"- If Pr(rank=1) is not dominant and ranks are spread, the 'overall winner' is uncertain.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19665506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def interpret_bf10(bf10: float) -> str:\n",
    "    \"\"\"\n",
    "    Common descriptive categories (Jeffreys-style heuristics).\n",
    "    \"\"\"\n",
    "    if bf10 < 1/10:\n",
    "        return \"strong evidence for H0 (no effect)\"\n",
    "    if bf10 < 1/3:\n",
    "        return \"moderate evidence for H0\"\n",
    "    if bf10 < 1:\n",
    "        return \"anecdotal evidence for H0\"\n",
    "    if bf10 < 3:\n",
    "        return \"anecdotal evidence for H1 (effect)\"\n",
    "    if bf10 < 10:\n",
    "        return \"moderate evidence for H1\"\n",
    "    if bf10 < 30:\n",
    "        return \"strong evidence for H1\"\n",
    "    if bf10 < 100:\n",
    "        return \"very strong evidence for H1\"\n",
    "    return \"extreme evidence for H1\"\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# interpret_bf10(12.5)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
