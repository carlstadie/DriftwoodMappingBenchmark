{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629364ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"yay\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from scipy.stats import gaussian_kde, norm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import scipy.stats as st\n",
    "from scipy.stats import gaussian_kde\n",
    "from typing import Sequence, Tuple, Dict, Optional, Callable\n",
    "\n",
    "print(\"Running with PyMC version:\", pm.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fda2add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths \n",
    "# here we have paths to folders where the logging histories of individual training runs are stored.\n",
    "\n",
    "# for Unet, Swin and Terramind trained on aerial, we have these logs:\n",
    "UNET_AE_METRICS = r\"N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\results\\UNET\\AE\\evaluation_unet.csv\"\n",
    "SWIN_AE_METRICS = r\"N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\results\\SWIN\\AE\\evaluation_swin.csv\"\n",
    "TERRAMIND_AE_METRICS = r\"N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\results\\Terramind\\AE\\evaluation_tm.csv\"\n",
    "\n",
    "# similarly for PlanetScope:\n",
    "UNET_PS_METRICS = r\"N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\results\\UNET\\PS\\evaluation_unet.csv\"\n",
    "SWIN_PS_METRICS = r\"N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\results\\SWIN\\PS\\evaluation_swin.csv\"\n",
    "TERRAMIND_PS_METRICS = r\"N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\results\\Terramind\\PS\\evaluation_tm.csv\"\n",
    "\n",
    "#and lastly for Sentinel 2\n",
    "UNET_S2_METRICS = r\"N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\results\\UNET\\S2\\evaluation_unet.csv\"\n",
    "SWIN_S2_METRICS = r\"N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\results\\SWIN\\S2\\evaluation_swin.csv\"\n",
    "TERRAMIND_S2_METRICS = r\"N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\results\\Terramind\\S2\\evaluation_tm.csv\"\n",
    "\n",
    "# metrics to analyse\n",
    "metrics = ['IoU', 'dice_coef', 'normalized_surface_distance', 'mean_epistemic_uncertainty', 'mean_aleatoric_uncertainty']\n",
    "\n",
    "# which metrics are \"higher is better\"\n",
    "maximize_metrics = {'IoU', 'dice_coef'}  # include accuracy too\n",
    "\n",
    "# metrics that are bounded in (0,1) and should be modelled on logit scale\n",
    "bounded_01_metrics = {'IoU', 'dice_coef', 'normalized_surface_distance', 'mean_epistemic_uncertainty', 'mean_aleatoric_uncertainty'}\n",
    "\n",
    "# priors for factorial effects (on transformed scale)\n",
    "# - logit scale: Normal(0, 0.5) is a reasonable weakly-informative prior\n",
    "# - log scale  : Normal(0, 0.2) corresponds to ~ +/- 22% multiplicative change (1 SD)\n",
    "prior_sd_logit = 0.5\n",
    "prior_sd_log = 0.2\n",
    "\n",
    "TEST = True\n",
    "\n",
    "# sampling config\n",
    "DRAWS = 2000\n",
    "TUNE = 2000\n",
    "CHAINS = 4\n",
    "TARGET_ACCEPT = 0.9\n",
    "HDI_PROB = 0.95 # use 95 as CI standard\n",
    "RANDOM_SEED = 1701\n",
    "\n",
    "# Savage–Dickey KDE bandwidth (can be adjusted)\n",
    "SD_KDE_BW = 0.3\n",
    "\n",
    "# overall ranking weights across metrics (explicit utility)\n",
    "# leave as None for equal weights across metrics\n",
    "metric_weights: Optional[Dict[str, float]] = None\n",
    "\n",
    "# output dirs (optional; keep yours)\n",
    "UNET_AE_OUTPUT_DIR = r\"N:\\isipd\\projects\\p_planetdw\\git\\DriftwoodMappingBenchmark\\figs\\stats\\UNET\\AE\"\n",
    "SWIN_AE_OUTPUT_DIR = r\"N:\\isipd\\projects\\p_planetdw\\git\\DriftwoodMappingBenchmark\\figs\\stats\\SWIN\\AE\"\n",
    "TERRAMIND_AE_OUTPUT_DIR = r\"N:\\isipd\\projects\\p_planetdw\\git\\DriftwoodMappingBenchmark\\figs\\stats\\TERRAMIND\\AE\"\n",
    "\n",
    "UNET_PS_OUTPUT_DIR = r\"N:\\isipd\\projects\\p_planetdw\\git\\DriftwoodMappingBenchmark\\figs\\stats\\UNET\\PS\"\n",
    "SWIN_PS_OUTPUT_DIR = r\"N:\\isipd\\projects\\p_planetdw\\git\\DriftwoodMappingBenchmark\\figs\\stats\\SWIN\\PS\"\n",
    "TERRAMIND_PS_OUTPUT_DIR = r\"N:\\isipd\\projects\\p_planetdw\\git\\DriftwoodMappingBenchmark\\figs\\stats\\TERRAMIND\\PS\"\n",
    "\n",
    "UNET_S2_OUTPUT_DIR = r\"N:\\isipd\\projects\\p_planetdw\\git\\DriftwoodMappingBenchmark\\figs\\stats\\UNET\\S2\"\n",
    "SWIN_S2_OUTPUT_DIR = r\"N:\\isipd\\projects\\p_planetdw\\git\\DriftwoodMappingBenchmark\\figs\\stats\\SWIN\\S2\"\n",
    "TERRAMIND_S2_OUTPUT_DIR = r\"N:\\isipd\\projects\\p_planetdw\\git\\DriftwoodMappingBenchmark\\figs\\stats\\TERRAMIND\\S2\"\n",
    "\n",
    "hdi_lower = (1.0 - HDI_PROB) / 2.0\n",
    "hdi_upper = 1.0 - hdi_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59c92b79",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# prepare\u001b[39;00m\n\u001b[32m     31\u001b[39m group_order = [\n\u001b[32m     32\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mU-Net | AE\u001b[39m\u001b[33m\"\u001b[39m,     \u001b[33m\"\u001b[39m\u001b[33mSwin U-Net | AE\u001b[39m\u001b[33m\"\u001b[39m,     \u001b[33m\"\u001b[39m\u001b[33mTerramind | AE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     33\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mU-Net | PS\u001b[39m\u001b[33m\"\u001b[39m,     \u001b[33m\"\u001b[39m\u001b[33mSwin U-Net | PS\u001b[39m\u001b[33m\"\u001b[39m,     \u001b[33m\"\u001b[39m\u001b[33mTerramind | PS\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     34\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mU-Net | S2\u001b[39m\u001b[33m\"\u001b[39m,     \u001b[33m\"\u001b[39m\u001b[33mSwin U-Net | S2\u001b[39m\u001b[33m\"\u001b[39m,     \u001b[33m\"\u001b[39m\u001b[33mTerramind | S2\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     35\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m all_data = \u001b[43mload_checkpoint_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCSV_SPECS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m all_data[\u001b[33m\"\u001b[39m\u001b[33mgroup\u001b[39m\u001b[33m\"\u001b[39m] = pd.Categorical(all_data[\u001b[33m\"\u001b[39m\u001b[33mgroup\u001b[39m\u001b[33m\"\u001b[39m], categories=group_order, ordered=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# for the last 3 entires in all_data, overwrite dataset with S2, regardless of the entry\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mload_checkpoint_metrics\u001b[39m\u001b[34m(csv_specs, metrics)\u001b[39m\n\u001b[32m     16\u001b[39m dfs = []\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path, group, dataset, arch \u001b[38;5;129;01min\u001b[39;00m csv_specs:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m.path.exists(path) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m     19\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWarning: path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist, skipping.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Map each CSV to its labels\n",
    "CSV_SPECS = [\n",
    "    (UNET_AE_METRICS,      \"U-Net | AE\",        \"AE\",   \"U-Net\"),\n",
    "    (SWIN_AE_METRICS,      \"Swin U-Net | AE\",   \"AE\",   \"Swin\"),\n",
    "    (TERRAMIND_AE_METRICS, \"Terramind | AE\",    \"AE\",   \"Terramind\"),\n",
    "    (UNET_PS_METRICS,      \"U-Net | PS\",        \"PS\",   \"U-Net\"),\n",
    "    (SWIN_PS_METRICS,      \"Swin U-Net | PS\",   \"PS\",   \"Swin\"),\n",
    "    (TERRAMIND_PS_METRICS, \"Terramind | PS\",    \"PS\",   \"Terramind\"),\n",
    "    (UNET_S2_METRICS,      \"U-Net | S2\",        \"S2\",   \"U-Net\"),\n",
    "    (SWIN_S2_METRICS,      \"Swin U-Net | S2\",   \"S2\",   \"Swin\"),\n",
    "    (TERRAMIND_S2_METRICS, \"Terramind | S2\",    \"S2\",   \"Terramind\"),\n",
    "]\n",
    "\n",
    "def load_checkpoint_metrics(csv_specs, metrics):\n",
    "    # metrics are column names like \"IoU\", \"dice_coef\", etc. (no val_ prefixes)\n",
    "    dfs = []\n",
    "    for path, group, dataset, arch in csv_specs:\n",
    "        if os.path.exists(path) is False:\n",
    "            print(f\"Warning: path {path} does not exist, skipping.\")\n",
    "            continue\n",
    "        df = pd.read_csv(path)\n",
    "        needed = [m for m in metrics if m in df.columns]\n",
    "        subset = df[needed].copy()\n",
    "        subset[\"group\"] = group\n",
    "        subset[\"dataset\"] = dataset\n",
    "        subset[\"arch\"] = arch\n",
    "        dfs.append(subset)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# prepare\n",
    "group_order = [\n",
    "    \"U-Net | AE\",     \"Swin U-Net | AE\",     \"Terramind | AE\",\n",
    "    \"U-Net | PS\",     \"Swin U-Net | PS\",     \"Terramind | PS\",\n",
    "    \"U-Net | S2\",     \"Swin U-Net | S2\",     \"Terramind | S2\",\n",
    "]\n",
    "all_data = load_checkpoint_metrics(CSV_SPECS, metrics)\n",
    "all_data[\"group\"] = pd.Categorical(all_data[\"group\"], categories=group_order, ordered=True)\n",
    "\n",
    "# for the last 3 entires in all_data, overwrite dataset with S2, regardless of the entry\n",
    "if TEST:\n",
    "    for i in range(len(all_data)-3, len(all_data)):\n",
    "        all_data.at[i, \"dataset\"] = \"S2\"\n",
    "\n",
    "display(all_data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3607dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We model bounded metrics (IoU, F1, sensitivity, ...) on logit scale.\n",
    "# We model positive metrics (loss, Hausdorff, ...) on log scale.\n",
    "# This avoids Normal likelihood pathologies at boundaries and ensures positivity.\n",
    "\n",
    "def base_metric_name(metric_col: str) -> str:\n",
    "    return metric_col[4:] if metric_col.startswith(\"val_\") else metric_col\n",
    "\n",
    "def is_bounded_01(metric_col: str) -> bool:\n",
    "    return base_metric_name(metric_col) in bounded_01_metrics\n",
    "\n",
    "def higher_is_better(metric_col: str) -> bool:\n",
    "    m = base_metric_name(metric_col)\n",
    "    return m in maximize_metrics\n",
    "\n",
    "def transform_y(y: np.ndarray, metric_col: str, eps: float = 1e-6) -> Tuple[np.ndarray, Callable[[np.ndarray], np.ndarray], str]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      y_transformed,\n",
    "      inverse_transform,\n",
    "      transform_name\n",
    "    \"\"\"\n",
    "    if is_bounded_01(metric_col):\n",
    "        y_clip = np.clip(y, eps, 1 - eps)\n",
    "        y_t = np.log(y_clip / (1 - y_clip))         # logit\n",
    "        inv = lambda z: 1 / (1 + np.exp(-z))\n",
    "        return y_t, inv, \"logit\"\n",
    "    else:\n",
    "        y_clip = np.clip(y, eps, None)\n",
    "        y_t = np.log(y_clip)                        # log\n",
    "        inv = lambda z: np.exp(z)\n",
    "        return y_t, inv, \"log\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38eae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_probabilities_from_draws(draws: np.ndarray, group_names, higher_better: bool) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    draws: (S, G) posterior draws of each group's performance (ORIGINAL scale preferred)\n",
    "    higher_better: True if larger = better, else smaller = better\n",
    "    \"\"\"\n",
    "    S, G = draws.shape\n",
    "    score = draws if higher_better else -draws\n",
    "\n",
    "    order = np.argsort(-score, axis=1)  # best..worst\n",
    "    ranks = np.empty_like(order)\n",
    "    for s in range(S):\n",
    "        ranks[s, order[s]] = np.arange(1, G + 1)\n",
    "\n",
    "    out = {\"group\": list(group_names)}\n",
    "    for k in range(1, G + 1):\n",
    "        out[f\"Pr(rank={k})\"] = [(ranks[:, j] == k).mean() for j in range(G)]\n",
    "    out[\"E[rank]\"] = [ranks[:, j].mean() for j in range(G)]\n",
    "    out[\"Pr(best)\"] = out[\"Pr(rank=1)\"]\n",
    "\n",
    "    return pd.DataFrame(out).sort_values(\"E[rank]\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef908f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BF10 = prior_density(Δ=0) / posterior_density(Δ=0)\n",
    "# Requires idata.prior draws for the effect parameter.\n",
    "\n",
    "def _density_at_zero(draws_1d: np.ndarray, bw=SD_KDE_BW) -> float:\n",
    "    draws_1d = np.asarray(draws_1d).ravel()\n",
    "    kde = gaussian_kde(draws_1d, bw_method=bw)\n",
    "    return float(kde.evaluate(0.0)[0])\n",
    "\n",
    "def savage_dickey_bf10(idata: az.InferenceData, var_name: str, bw=SD_KDE_BW) -> float:\n",
    "    \"\"\"\n",
    "    Savage–Dickey BF10 for H1: var != 0 vs H0: var = 0\n",
    "    BF10 = p(var=0 | prior) / p(var=0 | posterior)\n",
    "    \"\"\"\n",
    "    if not hasattr(idata, \"prior\"):\n",
    "        raise ValueError(\n",
    "            \"idata has no prior group. Ensure you ran pm.sample_prior_predictive(var_names=[...]) \"\n",
    "            \"and extended idata with it.\"\n",
    "        )\n",
    "\n",
    "    post = az.extract(idata, group=\"posterior\", var_names=[var_name]).to_numpy().ravel()\n",
    "    prior = az.extract(idata, group=\"prior\", var_names=[var_name]).to_numpy().ravel()\n",
    "\n",
    "    prior0 = _density_at_zero(prior, bw=bw)\n",
    "    post0  = _density_at_zero(post,  bw=bw)\n",
    "\n",
    "    return prior0 / post0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07c07ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_factorial_model_with_bf(\n",
    "    df: pd.DataFrame,\n",
    "    dataset_col: str,\n",
    "    arch_col: str,\n",
    "    metric_col: str,\n",
    "    draws=DRAWS,\n",
    "    tune=TUNE,\n",
    "    chains=CHAINS,\n",
    "    target_accept=TARGET_ACCEPT,\n",
    "    seed=RANDOM_SEED,\n",
    "    bw=SD_KDE_BW,\n",
    "    hdi_prob=HDI_PROB\n",
    "):\n",
    "    data = df[[dataset_col, arch_col, metric_col]].dropna().copy()\n",
    "    dcat = pd.Categorical(data[dataset_col]) # PS, S2 etc\n",
    "    acat = pd.Categorical(data[arch_col]) # Unet, Swin, Terramind\n",
    "    \n",
    "    len_datasets = len(dcat.categories)\n",
    "    len_archs = len(acat.categories)\n",
    "    \n",
    "    print(f\"Fitting a {len_datasets}×{len_archs} factorial model for '{metric_col}'\")\n",
    "    print(f\"Datasets: {list(dcat.categories)}\")\n",
    "    print(f\"Architectures: {list(acat.categories)}\")\n",
    "    \n",
    "    # Create effects-coded design matrices (sum-to-zero constraint)\n",
    "    # For K levels, use K-1 contrasts\n",
    "    d_idx = dcat.codes # eg ids for datasets o .. K-1\n",
    "    a_idx = acat.codes\n",
    "    n_obs = len(d_idx)\n",
    "    \n",
    "    # Effects coding: reference level gets -1, others get identity, one matrix each factor\n",
    "    D_mat = np.zeros((n_obs, len_datasets - 1)) \n",
    "    A_mat = np.zeros((n_obs, len_archs - 1))\n",
    "    \n",
    "    for i in range(n_obs):\n",
    "        # Fill in effects coding\n",
    "        if d_idx[i] < len_datasets - 1:\n",
    "            D_mat[i, d_idx[i]] = 1 # 1 means that level is an effect\n",
    "        else:  # reference level\n",
    "            D_mat[i, :] = -1 # all -1s for reference level\n",
    "\n",
    "        # same for architecture    \n",
    "        if a_idx[i] < len_archs - 1:\n",
    "            A_mat[i, a_idx[i]] = 1\n",
    "        else:  # reference level\n",
    "            A_mat[i, :] = -1\n",
    "        \n",
    "    \n",
    "    # Interaction design matrix: outer product structure\n",
    "    DA_mat = np.zeros((n_obs, (len_datasets - 1) * (len_archs - 1)))\n",
    "    col_idx = 0\n",
    "\n",
    "    # fill interaction matrix with effects coding\n",
    "    for d in range(len_datasets - 1):\n",
    "        for a in range(len_archs - 1):\n",
    "            DA_mat[:, col_idx] = D_mat[:, d] * A_mat[:, a]\n",
    "            col_idx += 1\n",
    "    \n",
    "    y_raw = data[metric_col].to_numpy()\n",
    "    y_t, inv, tname = transform_y(y_raw, metric_col) # transform to logit or log scale, to ensure Normal likelihood is appropriate\n",
    "    effect_sd = prior_sd_logit if tname == \"logit\" else prior_sd_log\n",
    "    \n",
    "    coords = {\n",
    "        \"dataset_effect\": list(dcat.categories)[:-1],\n",
    "        \"arch_effect\": list(acat.categories)[:-1],\n",
    "        \"interaction_effect\": [\n",
    "            f\"{dcat.categories[d]}×{acat.categories[a]}\"\n",
    "            for d in range(len_datasets - 1)\n",
    "            for a in range(len_archs - 1)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with pm.Model(coords=coords) as model:\n",
    "        # Data containers\n",
    "        D = pm.Data(\"D\", D_mat) # dataset factor\n",
    "        A = pm.Data(\"A\", A_mat) # architecture factor\n",
    "        DA = pm.Data(\"DA\", DA_mat) # interaction factor\n",
    "        \n",
    "        # Parameters as priors\n",
    "        intercept = pm.Normal(\"intercept\", 0.0, 1.5) # concentration around 0, with 1.5 sd\n",
    "        beta_dataset = pm.Normal(\"beta_dataset\", 0.0, effect_sd, \n",
    "                                dims=\"dataset_effect\") # prior say mean effect 0, sd depending on scale\n",
    "        beta_arch = pm.Normal(\"beta_arch\", 0.0, effect_sd, \n",
    "                            dims=\"arch_effect\") # same for architecture, no effect as prior \n",
    "        beta_interaction = pm.Normal(\"beta_interaction\", 0.0, effect_sd,\n",
    "                                    dims=\"interaction_effect\") # and we dont assume any interaction a priori\n",
    "        \n",
    "        # Linear predictor using matrix multiplication\n",
    "        mu = (intercept + \n",
    "              pm.math.dot(D, beta_dataset) + \n",
    "              pm.math.dot(A, beta_arch) + \n",
    "              pm.math.dot(DA, beta_interaction))\n",
    "        \n",
    "        # Robust likelihood\n",
    "        sigma = pm.HalfNormal(\"sigma\", 1.0)\n",
    "        nu = pm.Exponential(\"nu\", 1/30) + 1\n",
    "        pm.StudentT(\"y\", nu=nu, mu=mu, sigma=sigma, observed=y_t) # use student-t likelihood to be robust to outliers, although unlikely\n",
    "        \n",
    "        idata = pm.sample(\n",
    "            draws=draws,\n",
    "            tune=tune,\n",
    "            chains=chains,\n",
    "            target_accept=target_accept,\n",
    "            random_seed=seed,\n",
    "            return_inferencedata=True\n",
    "        )\n",
    "        \n",
    "        prior = pm.sample_prior_predictive(\n",
    "            var_names=[\"beta_dataset\", \"beta_arch\", \"beta_interaction\"],\n",
    "            random_seed=seed\n",
    "        )\n",
    "        idata.extend(prior) # add our prior samples to idata for BF calculation\n",
    "    \n",
    "    # Summarize results\n",
    "    summary = az.summary(\n",
    "        idata, \n",
    "        var_names=[\"beta_dataset\", \"beta_arch\", \"beta_interaction\"], \n",
    "        hdi_prob=HDI_PROB\n",
    "    ).reset_index()\n",
    "    \n",
    "    summary = summary.rename(columns={\"index\": \"param\"})\n",
    "    \n",
    "    # Add directional probabilities and BF for each parameter\n",
    "    # directional probabilities are defined as Pr(param > 0 | data)\n",
    "    for var_name in [\"beta_dataset\", \"beta_arch\", \"beta_interaction\"]:\n",
    "        param_draws = az.extract(idata, group=\"posterior\", var_names=[var_name])\n",
    "        \n",
    "        # Handle each dimension separately\n",
    "        if param_draws.ndim > 1:\n",
    "            for dim_val in param_draws.coords[param_draws.dims[0]].values:\n",
    "                draws = param_draws.sel({param_draws.dims[0]: dim_val}).values.ravel()\n",
    "                param_str = f\"{var_name}[{dim_val}]\"\n",
    "                mask = summary[\"param\"] == param_str\n",
    "                summary.loc[mask, \"Pr(>0)\"] = (draws > 0).mean()\n",
    "                # BF calculation would need to handle each dimension\n",
    "                #summary.loc[mask, \"BF10\"] = savage_dickey_bf10(idata, var_name, dim_val, bw=SD_KDE_BW)\n",
    "    \n",
    "    report = summary.loc[:, [\"param\", \"mean\", \"sd\", f\"hdi_{int(hdi_lower*100)}%\", f\"hdi_{int(hdi_upper*100)}%\", \"Pr(>0)\"]].copy()\n",
    "    report[\"scale\"] = tname\n",
    "    report[\"prior\"] = f\"Normal(0, {effect_sd}) on {tname} scale\"\n",
    "    \n",
    "    return idata, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a01ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_group_model_with_ranking(\n",
    "    df: pd.DataFrame,\n",
    "    group_col: str,\n",
    "    metric_col: str,\n",
    "    draws=DRAWS,\n",
    "    tune=TUNE,\n",
    "    chains=CHAINS,\n",
    "    target_accept=TARGET_ACCEPT,\n",
    "    seed=RANDOM_SEED\n",
    "):\n",
    "    import re\n",
    "    \n",
    "    data = df[[group_col, metric_col]].dropna().copy()\n",
    "\n",
    "    if pd.api.types.is_categorical_dtype(data[group_col]):\n",
    "        data[ group_col] = data[group_col].cat.remove_unused_categories()\n",
    "\n",
    "    gcat = pd.Categorical(data[group_col])\n",
    "    g_idx = gcat.codes\n",
    "    groups = list(gcat.categories)\n",
    "    G = len(groups)\n",
    "    y_raw = data[metric_col].to_numpy()\n",
    "    y_t, inv, tname = transform_y(y_raw, metric_col)\n",
    "    coords = {\"group\": groups}\n",
    "    \n",
    "    with pm.Model(coords=coords) as model:\n",
    "        g = pm.Data(\"g\", g_idx)\n",
    "        y_obs = pm.Data(\"y_obs\", y_t)\n",
    "        # hierarchical priors (transformed scale)\n",
    "        mu0 = pm.Normal(\"mu0\", 0.0, 1.5)\n",
    "        tau = pm.HalfNormal(\"tau\", 1.0)\n",
    "        mu = pm.Normal(\"mu\", mu0, tau, dims=\"group\")\n",
    "        sigma = pm.HalfNormal(\"sigma\", 1.0, dims=\"group\")\n",
    "        nu = pm.Exponential(\"nu\", 1/30) + 1\n",
    "        pm.StudentT(\"y\", nu=nu, mu=mu[g], sigma=sigma[g], observed=y_obs)\n",
    "        # deterministic group means on original scale\n",
    "        mu_orig = pm.Deterministic(\"mu_orig\", inv(mu), dims=\"group\")\n",
    "        \n",
    "        idata = pm.sample(\n",
    "            draws=draws,\n",
    "            tune=tune,\n",
    "            chains=chains,\n",
    "            target_accept=target_accept,\n",
    "            random_seed=seed,\n",
    "            return_inferencedata=True\n",
    "        )\n",
    "    \n",
    "    # posterior draws for ranking (original scale)\n",
    "    mu_draws = (\n",
    "        idata.posterior[\"mu_orig\"]\n",
    "        .stack(sample=(\"chain\",\"draw\"))\n",
    "        .transpose(\"sample\",\"group\")\n",
    "        .values\n",
    "    )\n",
    "    hib = higher_is_better(metric_col)\n",
    "    rank_table = rank_probabilities_from_draws(mu_draws, groups, higher_better=hib)\n",
    "    \n",
    "    # summary table (original scale)\n",
    "    summ = az.summary(idata, var_names=[\"mu_orig\"], hdi_prob=0.94).reset_index()\n",
    "    summ = summ.rename(columns={\"index\": \"param\"})\n",
    "    \n",
    "    # Extract group name from parameter string (handles labeled coordinates)\n",
    "    summ[\"group\"] = summ[\"param\"].str.replace(r\"mu_orig\\[|\\]\", \"\", regex=True)\n",
    "    \n",
    "    perf_table = (\n",
    "        summ.loc[:, [\"group\", \"mean\", \"sd\", \"hdi_3%\", \"hdi_97%\"]]\n",
    "        .merge(rank_table, on=\"group\", how=\"left\")\n",
    "        .sort_values(\"E[rank]\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    return idata, perf_table, rank_table, mu_draws, tname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eff4190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a composite score:\n",
    "#  - for each metric, ensure \"higher is better\" by sign-flip if necessary\n",
    "#  - standardise per metric so scales don't dominate\n",
    "#  - weighted average across metrics (equal weights by default)\n",
    "#\n",
    "# Then compute posterior rank probabilities from composite draws.\n",
    "\n",
    "def overall_ranking_across_metrics(\n",
    "    mu_draws_by_metric: Dict[str, np.ndarray],\n",
    "    higher_is_better_by_metric: Dict[str, bool],\n",
    "    group_names: Sequence[str],\n",
    "    weights: Optional[Dict[str, float]] = None,\n",
    "):\n",
    "    metric_list = list(mu_draws_by_metric.keys())\n",
    "\n",
    "    if weights is None:\n",
    "        weights = {m: 1.0 for m in metric_list}\n",
    "    wsum = sum(weights.values())\n",
    "    weights = {m: weights[m] / wsum for m in metric_list}\n",
    "\n",
    "    # align draws\n",
    "    S = min(mu_draws_by_metric[m].shape[0] for m in metric_list)\n",
    "    G = len(group_names)\n",
    "\n",
    "    composite = np.zeros((S, G))\n",
    "    for m in metric_list:\n",
    "        mu = mu_draws_by_metric[m][:S, :]\n",
    "        score = mu if higher_is_better_by_metric[m] else -mu\n",
    "\n",
    "        # standardize to avoid scale dominance\n",
    "        sd = score.std() + 1e-12\n",
    "        z = score / sd\n",
    "\n",
    "        composite += weights[m] * z\n",
    "\n",
    "    overall_rank_table = rank_probabilities_from_draws(composite, group_names, higher_better=True)\n",
    "\n",
    "    return composite, overall_rank_table, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78a12d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For each metric:\n",
    "#  1) extract per-run \"best checkpoint\" values for 4 combinations\n",
    "#  2) build dataframe with factors (group/dataset/arch)\n",
    "#  3) plot distribution\n",
    "#  4) fit:\n",
    "#     - hierarchical group model (ranking)\n",
    "#     - factorial model (effects + Savage–Dickey BF10)\n",
    "#  5) store posterior draws for overall multi-metric ranking\n",
    "\n",
    "tabs = []\n",
    "tab_titles = []\n",
    "\n",
    "# store posterior group-mean draws per metric for overall ranking\n",
    "mu_draws_by_metric: Dict[str, np.ndarray] = {}\n",
    "hib_by_metric: Dict[str, bool] = {}\n",
    "\n",
    "for metric in metrics:\n",
    "\n",
    "    out = widgets.Output()\n",
    "\n",
    "    with out:\n",
    "\n",
    "        if TEST:\n",
    "            print(\"TESTMODE, LAST 3 ROWS SET TO S2 DATASET, DO NOT USE FOR REAL ANALYSIS\")\n",
    "            \n",
    "        clear_output(wait=True)\n",
    "        print(f'Processing: {metric}\\n')\n",
    "\n",
    "        data = all_data[['group', 'dataset', 'arch', metric]].dropna().copy()\n",
    "\n",
    "        # visualize run-to-run distribution\n",
    "        sns.kdeplot(data=data, x=metric, hue=\"group\")\n",
    "        plt.title(f\"Distribution across runs: {metric}\")\n",
    "        plt.show()\n",
    "\n",
    "        # 1) hierarchical group model -> rank probabilities\n",
    "        idata_g, perf_table, rank_table, mu_draws, scale_name = fit_group_model_with_ranking(\n",
    "            data, group_col=\"group\", metric_col=metric\n",
    "        )\n",
    "\n",
    "        print(\"\\n--- Hierarchical group model (group means on original scale) ---\")\n",
    "        print(f\"Modelled on {scale_name} scale; reporting mu_orig on original scale.\\n\")\n",
    "        display(perf_table)\n",
    "\n",
    "        # store for overall ranking\n",
    "        mu_draws_by_metric[metric] = mu_draws\n",
    "        hib_by_metric[metric] = higher_is_better(metric)\n",
    "\n",
    "        # 2) factorial model -> effect size + uncertainty + BF10 (Savage–Dickey)\n",
    "        idata_f, factorial_report = fit_factorial_model_with_bf(\n",
    "            data, dataset_col=\"dataset\", arch_col=\"arch\", metric_col=metric\n",
    "        )\n",
    "\n",
    "        print(\"\\n--- Factorial effects (transformed scale) + Savage–Dickey BF10 ---\")\n",
    "        display(factorial_report)\n",
    "\n",
    "    tabs.append(out)\n",
    "    tab_titles.append(metric)\n",
    "\n",
    "tab_widget = widgets.Tab(children=tabs)\n",
    "for i, title in enumerate(tab_titles):\n",
    "    tab_widget.set_title(i, title)\n",
    "\n",
    "display(tab_widget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db4c343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This produces a single posterior ranking of the 4 combinations \"overall\".\n",
    "# IMPORTANT: \"overall\" depends on the utility you define (weights + standardization).\n",
    "# Default: equal weights.\n",
    "\n",
    "composite_draws, overall_rank_table, used_weights = overall_ranking_across_metrics(\n",
    "    mu_draws_by_metric=mu_draws_by_metric,\n",
    "    higher_is_better_by_metric=hib_by_metric,\n",
    "    group_names=group_order,\n",
    "    weights=metric_weights\n",
    ")\n",
    "\n",
    "print(\"\\n===============================\")\n",
    "print(\"OVERALL MULTI-METRIC RANKING\")\n",
    "print(\"===============================\\n\")\n",
    "\n",
    "display(overall_rank_table)\n",
    "\n",
    "print(\"\\nWeights used:\")\n",
    "print(used_weights)\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Pr(rank=1) is the posterior probability a combination is best overall under this utility.\")\n",
    "print(\"- E[rank] is the expected rank (lower = better).\")\n",
    "print(\"- If Pr(rank=1) is not dominant and ranks are spread, the 'overall winner' is uncertain.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19665506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def interpret_bf10(bf10: float) -> str:\n",
    "    \"\"\"\n",
    "    Common descriptive categories (Jeffreys-style heuristics).\n",
    "    \"\"\"\n",
    "    if bf10 < 1/10:\n",
    "        return \"strong evidence for H0 (no effect)\"\n",
    "    if bf10 < 1/3:\n",
    "        return \"moderate evidence for H0\"\n",
    "    if bf10 < 1:\n",
    "        return \"anecdotal evidence for H0\"\n",
    "    if bf10 < 3:\n",
    "        return \"anecdotal evidence for H1 (effect)\"\n",
    "    if bf10 < 10:\n",
    "        return \"moderate evidence for H1\"\n",
    "    if bf10 < 30:\n",
    "        return \"strong evidence for H1\"\n",
    "    if bf10 < 100:\n",
    "        return \"very strong evidence for H1\"\n",
    "    return \"extreme evidence for H1\"\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# interpret_bf10(12.5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
