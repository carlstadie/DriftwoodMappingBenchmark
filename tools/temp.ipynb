{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b4943e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from scipy.stats import gaussian_kde, norm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "print(\"Running with PyMC version:\", pm.__version__)\n",
    "\n",
    "data = pd.read_table(r\"N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\s2_outputs\\SP026_ind-hypersp.tab\")\n",
    "data\n",
    "\n",
    "plt.plot(data['Lambda [nm]'], data['Refl [%]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ba7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import pandas as pd\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "DIR = r\"C:\\Users\\castad001\\Downloads\\P5-258_PERMA-X_2025_mastertracks\\datasets\"\n",
    "\n",
    "file_list = [f for f in os.listdir(DIR) if f.endswith('.tab')]\n",
    "\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "for file in file_list:\n",
    "    file_path = os.path.join(DIR, file)\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=17)\n",
    "    merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "\n",
    "\n",
    "# make to a geopandas dataframe\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    merged_df, geometry=gpd.points_from_xy(merged_df['Longitude'], merged_df['Latitude']), crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# make points of every day to a line\n",
    "gdf['Date'] = pd.to_datetime(gdf['Date/Time'])\n",
    "gdf = gdf.sort_values(by=['Date'])\n",
    "lines = gdf.groupby(gdf['Date'].dt.date).apply(\n",
    "    lambda x: LineString(x.geometry.tolist())\n",
    ")\n",
    "\n",
    "lines_gdf = gpd.GeoDataFrame({'geometry': lines}, crs=\"EPSG:4326\")\n",
    "#lines_gdf['Date'] = pd.to_datetime(lines_gdf.index)\n",
    "\n",
    "lines_gdf.to_file(r\"C:\\Users\\castad001\\Downloads\\P5-258_PERMA-X_2025_mastertracks\\datasets\\mastertracks_lines.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a5344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import geopandas as gpd\n",
    "\n",
    "# --- Load ---\n",
    "csv_path = r\"\\\\smb.isipd.dmawi.de\\projects\\p_planetdw\\data\\methods_test\\aois\\S2_GEE_export_summary.csv\"\n",
    "gpkg_path = r\"\\\\smb.isipd.dmawi.de\\projects\\p_planetdw\\data\\methods_test\\auxilliary_data\\aoi_ext.gpkg\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Only need attributes (layer, target_name, time)\n",
    "aerialaq = gpd.read_file(gpkg_path, ignore_geometry=True)\n",
    "\n",
    "# --- Aerial: build full datetime (date from 'layer', time from 'time' column) ---\n",
    "\n",
    "# Date from layer: 3rd part, YYYYMMDD\n",
    "aerialaq[\"Date\"] = aerialaq[\"layer\"].str.split(\"_\").str[2]\n",
    "aerialaq[\"Date\"] = pd.to_datetime(aerialaq[\"Date\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "\n",
    "# Time string (Zulu HH:MM)\n",
    "aerialaq[\"time\"] = aerialaq[\"time\"].astype(str).str.strip()\n",
    "\n",
    "# Combine date + time into full datetime, then make it naive UTC (no tz info)\n",
    "aerialaq[\"DateTime\"] = pd.to_datetime(\n",
    "    aerialaq[\"Date\"].dt.strftime(\"%Y-%m-%d\") + \" \" + aerialaq[\"time\"],\n",
    "    utc=True,\n",
    "    errors=\"coerce\",\n",
    ").dt.tz_convert(None)\n",
    "\n",
    "# --- Sentinel (and others): parse datetime with time-of-day ---\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], utc=True, errors=\"coerce\").dt.tz_convert(None)\n",
    "\n",
    "# Make sure DateDeltaDays is numeric (used for Aerial offset)\n",
    "df[\"DateDeltaDays\"] = pd.to_numeric(df.get(\"DateDeltaDays\", 0), errors=\"coerce\").fillna(0)\n",
    "\n",
    "# Infer Sensor if not present\n",
    "def infer_sensor(row):\n",
    "    s = \" \".join(str(row.get(c, \"\")) for c in [\"ExportTask\", \"SystemIndex\"]).upper()\n",
    "    if (\"_AE\" in s) or (\"AE_\" in s) or (\" AE \" in s):\n",
    "        return \"AE\"\n",
    "    if \"AERIAL\" in s:\n",
    "        return \"Aerial\"\n",
    "    if (\"S2\" in s) or (\"SENTINEL-2\" in s) or (\"SENTINEL2\" in s):\n",
    "        return \"Sentinel-2\"\n",
    "    return \"Other\"\n",
    "\n",
    "if \"Sensor\" not in df.columns:\n",
    "    df[\"Sensor\"] = df.apply(infer_sensor, axis=1)\n",
    "\n",
    "# --- Plot-time date for each row ---\n",
    "# Sentinel-2 / AE → use Date\n",
    "# Aerial (from CSV) → Date - DateDeltaDays (still allowed, but now with full datetime)\n",
    "df[\"plot_date\"] = df[\"Date\"]\n",
    "is_aerial_csv = df[\"Sensor\"].eq(\"Aerial\") & df[\"Date\"].notna()\n",
    "df.loc[is_aerial_csv, \"plot_date\"] = df.loc[is_aerial_csv, \"Date\"] - pd.to_timedelta(\n",
    "    df.loc[is_aerial_csv, \"DateDeltaDays\"], unit=\"D\"\n",
    ")\n",
    "\n",
    "# --- Window: fixed for all AOIs ---\n",
    "window_start = pd.Timestamp(\"2025-07-01 00:00\")\n",
    "window_end   = pd.Timestamp(\"2025-08-31 23:59\")\n",
    "\n",
    "# Keep only markers that fall inside the window\n",
    "df_plot = df[df[\"plot_date\"].between(window_start, window_end)].copy()\n",
    "\n",
    "# --- AOI layout ---\n",
    "aoi_order = df_plot[\"AOI\"].dropna().drop_duplicates().tolist()\n",
    "aoi_to_y = {aoi: i for i, aoi in enumerate(aoi_order)}\n",
    "\n",
    "# --- Figure ---\n",
    "fig_h = max(4, 0.5 * len(aoi_order))\n",
    "fig, ax = plt.subplots(figsize=(12, fig_h))\n",
    "\n",
    "# Light-grey 10-pt line for each AOI across the fixed window\n",
    "for aoi in aoi_order:\n",
    "    y = aoi_to_y[aoi]\n",
    "    ax.hlines(\n",
    "        y=y,\n",
    "        xmin=window_start,\n",
    "        xmax=window_end,\n",
    "        color=\"lightgrey\",\n",
    "        linewidth=10,\n",
    "        alpha=0.85,\n",
    "        zorder=1,\n",
    "    )\n",
    "\n",
    "# Colors per sensor\n",
    "sensor_colors = {\n",
    "    \"Sentinel-2\": \"#1f77b4\",  # blue\n",
    "    \"Aerial\": \"#ff7f0e\",      # orange\n",
    "    \"AE\": \"#2ca02c\",          # green\n",
    "    \"Other\": \"#7f7f7f\",       # grey\n",
    "}\n",
    "\n",
    "legend_done = set()\n",
    "\n",
    "# --- Plot Sentinel / AE / (CSV) Aerial as '|' markers with full datetime ---\n",
    "for _, row in df_plot.dropna(subset=[\"plot_date\", \"AOI\"]).iterrows():\n",
    "    y = aoi_to_y.get(row[\"AOI\"])\n",
    "    x = row[\"plot_date\"]\n",
    "    sensor = row.get(\"Sensor\", \"Other\")\n",
    "    color = sensor_colors.get(sensor, sensor_colors[\"Other\"])\n",
    "    label = sensor if sensor not in legend_done else \"_nolegend_\"\n",
    "\n",
    "    ax.plot(\n",
    "        x,\n",
    "        y,\n",
    "        marker=\"|\",\n",
    "        markersize=18,\n",
    "        markeredgewidth=2,\n",
    "        linestyle=\"None\",\n",
    "        markerfacecolor=\"none\",\n",
    "        markeredgecolor=color,\n",
    "        zorder=3,\n",
    "        label=label,\n",
    "    )\n",
    "    legend_done.add(sensor)\n",
    "\n",
    "# --- Aerial AOI points (from GPKG) as orange '|' markers with time-of-day ---\n",
    "aerial_points = aerialaq[aerialaq[\"DateTime\"].between(window_start, window_end)].copy()\n",
    "aerial_points[\"Sensor\"] = \"Aerial\"\n",
    "\n",
    "for _, row in aerial_points.dropna(subset=[\"DateTime\", \"target_name\"]).iterrows():\n",
    "    aoi = row[\"target_name\"]\n",
    "    if aoi not in aoi_to_y:\n",
    "        continue  # skip AOIs not in the Sentinel CSV\n",
    "\n",
    "    x = row[\"DateTime\"]\n",
    "    y = aoi_to_y[aoi]\n",
    "    sensor = \"Aerial\"\n",
    "    color = sensor_colors[\"Aerial\"]\n",
    "    label = sensor if sensor not in legend_done else \"_nolegend_\"\n",
    "\n",
    "    ax.plot(\n",
    "        x,\n",
    "        y,\n",
    "        marker=\"|\",\n",
    "        markersize=18,\n",
    "        markeredgewidth=2,\n",
    "        linestyle=\"None\",\n",
    "        markerfacecolor=\"none\",\n",
    "        markeredgecolor=color,\n",
    "        zorder=3,\n",
    "        label=label,\n",
    "    )\n",
    "    legend_done.add(sensor)\n",
    "\n",
    "# --- Axes/formatting ---\n",
    "ax.set_yticks([aoi_to_y[a] for a in aoi_order])\n",
    "ax.set_yticklabels(aoi_order)\n",
    "ax.invert_yaxis()  # optional\n",
    "\n",
    "ax.set_xlim(window_start, window_end)\n",
    "ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "ax.xaxis.set_major_formatter(\n",
    "    mdates.ConciseDateFormatter(ax.xaxis.get_major_locator())\n",
    ")\n",
    "plt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\")\n",
    "\n",
    "ax.set_xlabel(\"Acquisition datetime (UTC)\")\n",
    "ax.set_ylabel(\"AOI\")\n",
    "ax.set_title(\"Acquisitions per AOI (| = acquisition; 2025-07-01 to 2025-08-31, with time-of-day)\")\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "if labels:\n",
    "    ax.legend(handles, labels, title=\"Sensor\", loc=\"upper right\", frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =====================================================================\n",
    "#   STATISTICS: time difference between acquisitions (per AOI)\n",
    "# =====================================================================\n",
    "\n",
    "# Build a combined acquisitions table (Sentinel + AE + Aerial CSV + Aerial GPKG)\n",
    "acq_list = []\n",
    "\n",
    "# From CSV\n",
    "tmp_csv = df_plot.dropna(subset=[\"plot_date\", \"AOI\"])[[\"AOI\", \"Sensor\", \"plot_date\"]].copy()\n",
    "acq_list.append(tmp_csv)\n",
    "\n",
    "# From GPKG (Aerial AOI)\n",
    "tmp_aerial = aerial_points.dropna(subset=[\"DateTime\", \"target_name\"])[\n",
    "    [\"target_name\", \"Sensor\", \"DateTime\"]\n",
    "].copy()\n",
    "tmp_aerial = tmp_aerial.rename(columns={\"target_name\": \"AOI\", \"DateTime\": \"plot_date\"})\n",
    "acq_list.append(tmp_aerial)\n",
    "\n",
    "acq_all = pd.concat(acq_list, ignore_index=True)\n",
    "\n",
    "# Sort by AOI + datetime\n",
    "acq_all = acq_all.sort_values([\"AOI\", \"plot_date\"])\n",
    "\n",
    "# Time difference to previous acquisition in the SAME AOI (in hours)\n",
    "acq_all[\"time_diff_hours\"] = (\n",
    "    acq_all.groupby(\"AOI\")[\"plot_date\"].diff().dt.total_seconds() / 3600.0\n",
    ")\n",
    "\n",
    "# Summary statistics per AOI\n",
    "stats = (\n",
    "    acq_all.dropna(subset=[\"time_diff_hours\"])\n",
    "    .groupby(\"AOI\")[\"time_diff_hours\"]\n",
    "    .agg(\n",
    "        count=\"count\",\n",
    "        mean_hours=\"mean\",\n",
    "        median_hours=\"median\",\n",
    "        min_hours=\"min\",\n",
    "        max_hours=\"max\",\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Print nice table\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:6.2f}\")\n",
    "print(\"\\nTime difference between consecutive acquisitions per AOI (hours):\")\n",
    "print(stats.to_string(index=False))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944d6072",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_list = [True, True, True, True]\n",
    "\n",
    "input_channels = list(range(len(channel_list)))\n",
    "label_channel = len(channel_list)  # label directly after inputs\n",
    "\n",
    "print(input_channels, label_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d00fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_macs_loss_dir = r'N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\logs\\unet_ae_samples'\n",
    "swin_macs_loss_dir = r'N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\logs\\swin_ae_samples'\n",
    "\n",
    "unet_ps_loss_dir = r'N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\logs\\unet_ps_samples'\n",
    "swin_ps_loss_dir = r'N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\logs\\swin_ps_samples'\n",
    "\n",
    "metrics = ['loss', 'specificity', 'sensitivity', 'IoU', 'f1_score', 'Hausdorff_distance']\n",
    "maximize_metrics = {'specificity', 'sensitivity', 'IoU', 'f1_score'}\n",
    "\n",
    "unet_macs_output_dir = r'N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\logs\\unet_ae_samples'\n",
    "swin_macs_output_dir = r'N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\logs\\swin_ae_samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e78c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metrics_as_array(directory, metrics):\n",
    "    files = sorted([f for f in os.listdir(directory) if f.endswith('.csv')])\n",
    "    data_list = []\n",
    "    \n",
    "    metric_names = []\n",
    "    for metric in metrics:\n",
    "        metric_names.append(metric)\n",
    "        metric_names.append('val_' + metric)\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        file_data = []\n",
    "\n",
    "        for name in metric_names:\n",
    "            if name in df.columns:\n",
    "                file_data.append(df[name].values)\n",
    "            else:\n",
    "                # Fill with NaNs if column is missing\n",
    "                file_data.append(np.full(len(df), np.nan))\n",
    "        \n",
    "        # Transpose so shape is (epochs, metrics)\n",
    "        file_data = np.stack(file_data, axis=1)  # shape: (epochs, num_metrics)\n",
    "        data_list.append(file_data)\n",
    "\n",
    "    # Convert to a 3D array: (files, epochs, metrics)\n",
    "    data_array = np.stack(data_list, axis=0)\n",
    "\n",
    "    # Build lookup dict\n",
    "    lookup = {name: idx for idx, name in enumerate(metric_names)}\n",
    "\n",
    "    return data_array, lookup, files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c41dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(loss_array, metrics, metric_lookup, output_dir, show_plot=False):\n",
    "    epochs = loss_array.shape[1]\n",
    "    num_metrics = len(metrics)\n",
    "\n",
    "    plt.figure(figsize=(20, 2.5))  # Square layout\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(1, num_metrics, i + 1)\n",
    "\n",
    "        # Training metric\n",
    "        for j in range(loss_array.shape[0]):\n",
    "            plt.plot(range(epochs), loss_array[j, :, metric_lookup[metric]], color='lightblue', linewidth=1)\n",
    "\n",
    "        train_mean = np.nanmean(loss_array[:, :, metric_lookup[metric]], axis=0)\n",
    "        \n",
    "        # Validation metric\n",
    "        val_metric = 'val_' + metric\n",
    "        if val_metric in metric_lookup:\n",
    "            for j in range(loss_array.shape[0]):\n",
    "                plt.plot(range(epochs), loss_array[j, :, metric_lookup[val_metric]], color='peachpuff', linewidth=1)\n",
    "\n",
    "            val_mean = np.nanmean(loss_array[:, :, metric_lookup[val_metric]], axis=0)\n",
    "            plt.plot(range(epochs), train_mean, color='tab:blue', label=f'train', linewidth=2)\n",
    "            plt.plot(range(epochs), val_mean, color='tab:orange', label=f'val', linewidth=2)\n",
    "\n",
    "        plt.title(metric)\n",
    "        plt.ylim(0,1)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel(metric)\n",
    "        #plt.legend(loc='upper right')\n",
    "        plt.grid(True)\n",
    "        plt.gca().set_aspect('auto')  # Square plot per metric (approx)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    output_path = os.path.join(output_dir, 'losses_plot.png')\n",
    "    plt.savefig(output_path)\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to {output_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e2c257",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_macs, unet_macs_metric_lookup, unet_macs_file_names = read_metrics_as_array(unet_macs_loss_dir, metrics)\n",
    "swin_macs, swin_macs_metric_lookup, swin_macs_file_names = read_metrics_as_array(swin_macs_loss_dir, metrics)\n",
    "unet_ps, unet_ps_metric_lookup, unet_ps_file_names = read_metrics_as_array(unet_ps_loss_dir, metrics)\n",
    "swin_ps, swin_ps_metric_lookup, swin_ps_file_names = read_metrics_as_array(swin_ps_loss_dir, metrics)\n",
    "\n",
    "plot_losses(unet_macs, metrics, unet_macs_metric_lookup, unet_macs_output_dir, show_plot=True)\n",
    "plot_losses(swin_macs, metrics, swin_macs_metric_lookup, swin_macs_output_dir, show_plot=True)\n",
    "plot_losses(unet_ps, metrics, unet_ps_metric_lookup, unet_ps_loss_dir, show_plot=True)\n",
    "plot_losses(swin_ps, metrics, swin_ps_metric_lookup, swin_ps_loss_dir, show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2805005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric(data_array, metric_lookup, metric):\n",
    "    \"\"\"\n",
    "    Get the best metric values across epochs for each file in the data array.\n",
    "    Args:\n",
    "        data_array (np.ndarray): 3D array of shape (files, epochs, metrics).\n",
    "        metric_lookup (dict): Dictionary mapping metric names to their indices.\n",
    "        metric (str): The metric to evaluate.\n",
    "        maximize_metrics (set): Set of metrics that should be maximized.\n",
    "    Returns:\n",
    "        np.ndarray: Array of best metric values for each file.\n",
    "    \"\"\"\n",
    "\n",
    "    best_values = []\n",
    "\n",
    "\n",
    "    for i in range(data_array.shape[0]):\n",
    "\n",
    "        losses = data_array[i, :, metric_lookup['loss']]\n",
    "        values = data_array[i, :, metric_lookup[metric]]\n",
    "\n",
    "\n",
    "        best_epoch = np.nanargmin(losses)\n",
    "        #print(f'best epoch; {best_epoch}')\n",
    "\n",
    "        best_value = values[best_epoch]\n",
    "        #print('best value:', best_value)\n",
    "\n",
    "        best_values.append(best_value)\n",
    "\n",
    "    return np.array(best_values)\n",
    "\n",
    "def interpret_bayes_factor(bf):\n",
    "    \"\"\"Return Jeffreys-style verbal label for a Bayes factor > 1.\"\"\"\n",
    "    if bf < 3:\n",
    "        return \"anecdotal\"\n",
    "    elif bf < 10:\n",
    "        return \"moderate\"\n",
    "    elif bf < 30:\n",
    "        return \"strong\"\n",
    "    elif bf < 100:\n",
    "        return \"very strong\"\n",
    "    else:\n",
    "        return \"extreme\"\n",
    "\n",
    "def BEST(combined_array, group_one, group_two, metric, minimal_metrcis = ['val_loss', 'loss', 'val_Hausdorff_distance', 'Hausdorff_distance'], group_one_label='unet', group_two_label='swin', plot=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Perform Bayesian estimation of the difference in means and standard deviations\n",
    "    between two groups using Student's t-distribution (Krischke 2005).\n",
    "    Args:\n",
    "        combined_array (pd.DataFrame): DataFrame containing the metric values and group labels.\n",
    "        group_one (np.ndarray): values of group 1.\n",
    "        group_two (np.ndarray): values of group 2.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    metric_values = combined_array[metric].values\n",
    "\n",
    "    mu_m = metric_values.mean()\n",
    "    mu_s = metric_values.std()*2\n",
    "\n",
    "    sigma_low = 10**-1\n",
    "    sigma_high = 10\n",
    "\n",
    "    with pm.Model() as model:\n",
    "        group1_mean = pm.Normal(f'{group_one_label}_mean', mu=mu_m, sigma=mu_s)\n",
    "        group2_mean = pm.Normal(f'{group_two_label}_mean', mu=mu_m, sigma=mu_s)\n",
    "\n",
    "        group1_std = pm.Uniform(f'{group_one_label}_std', lower=sigma_low, upper=sigma_high)\n",
    "        group2_std = pm.Uniform(f'{group_two_label}_std', lower=sigma_low, upper=sigma_high)\n",
    "\n",
    "        nu_minus_one = pm.Exponential('nu_minus_one', lam=1/29)\n",
    "        nu = pm.Deterministic('nu', nu_minus_one + 1)\n",
    "        nu_log10 = pm.Deterministic('nu_log10', np.log10(nu))\n",
    "\n",
    "        lambda_group1 = group1_std**-2\n",
    "        lambda_group2 = group2_std**-2\n",
    "\n",
    "        group_one_obs = pm.StudentT(f'{group_one_label}_obs', mu=group1_mean, lam=lambda_group1, nu=nu, observed=group_one)\n",
    "        group_two_obs = pm.StudentT(f'{group_two_label}_obs', mu=group2_mean, lam=lambda_group2, nu=nu, observed=group_two)\n",
    "\n",
    "        diff_of_means = pm.Deterministic('diff_of_means', group1_mean - group2_mean)\n",
    "        diff_of_stds = pm.Deterministic('diff_of_stds', group1_std - group2_std)\n",
    "        effect_size = pm.Deterministic('effect_size', diff_of_means / np.sqrt((group1_std**2 + group2_std**2) / 2))\n",
    "\n",
    "        idata = pm.sample(tune=1000, draws=2000, chains=4, target_accept=0.95, return_inferencedata=True)\n",
    "\n",
    "    if plot:\n",
    "        print('\\n---- Posterior for the means and stds ----')\n",
    "\n",
    "        az.plot_posterior(idata, var_names=[f'{group_one_label}_mean', f'{group_two_label}_mean', f'{group_one_label}_std', f'{group_two_label}_std', 'nu_log10', 'nu'])\n",
    "        plt.show()\n",
    "\n",
    "        print('\\n---- Posterior for the differences and effect size ----')\n",
    "\n",
    "        az.plot_posterior(idata, var_names=['diff_of_means', 'diff_of_stds', 'effect_size'], ref_val=0)\n",
    "        plt.show()\n",
    "\n",
    "        print('\\n---- Forests for means, stds, and nu ----')\n",
    "        \n",
    "        az.plot_forest(idata, var_names=[f'{group_one_label}_mean', f'{group_two_label}_mean'])\n",
    "        plt.show()\n",
    "\n",
    "        print('\\n---- Forests for stds and nu ----')\n",
    "\n",
    "        az.plot_forest(idata, var_names=[f'{group_one_label}_std', f'{group_two_label}_std', 'nu'])\n",
    "        plt.show()\n",
    "\n",
    "    print('\\n---- Model summary ----')\n",
    "\n",
    "    summary = az.summary(idata, var_names=[f'{group_one_label}_mean', f'{group_two_label}_mean','diff_of_means', 'diff_of_stds', 'effect_size'])\n",
    "    print(summary)\n",
    "\n",
    "    print('\\n---- Savage-Dickey Bayes Factor ----\\n')\n",
    "\n",
    "    \n",
    "    # Posterior density at δ = 0  (KDE is still appropriate here)\n",
    "    diff_samples = idata.posterior['diff_of_means'].values.flatten()\n",
    "    posterior_kde            = gaussian_kde(diff_samples)\n",
    "    posterior_density_at_zero = posterior_kde.evaluate(0)[0]\n",
    "\n",
    "    # Analytical prior density at δ = 0  (δ ~ Normal(0, √2·mu_s))\n",
    "    prior_sd_diff            = np.sqrt(2) * mu_s\n",
    "    prior_density_at_zero    = norm.pdf(0, loc=0, scale=prior_sd_diff)\n",
    "\n",
    "    # Bayes factors\n",
    "    BF_01 = posterior_density_at_zero / prior_density_at_zero   # H₀ over H₁\n",
    "    BF_10 = 1 / BF_01                                           # H₁ over H₀\n",
    "\n",
    "    metric_is_lower_better = metric  in minimal_metrcis\n",
    "    mean_diff = diff_samples.mean()   # μ_unet − μ_swin\n",
    "\n",
    "    if BF_10 > 1:            # data support the alternative\n",
    "        # who wins, given the metric direction?\n",
    "        if (not metric_is_lower_better and mean_diff > 0) or \\\n",
    "        (    metric_is_lower_better and mean_diff < 0):\n",
    "            winner, loser = group_one_label, group_two_label\n",
    "        else:\n",
    "            winner, loser = group_two_label, group_one_label\n",
    "\n",
    "        label = interpret_bayes_factor(BF_10)\n",
    "        print(f\"p(δ=0)        : {prior_density_at_zero:.4g}\")\n",
    "        print(f\"p(δ=0 | data) : {posterior_density_at_zero:.4g}\")\n",
    "        print(f\"Evidence for {winner} outperforming {loser}: \"\n",
    "            f\"BF₁₀ = {BF_10:.4g}  ({label})\\n\")\n",
    "\n",
    "    else:                     # data support the null\n",
    "        label = interpret_bayes_factor(BF_01)\n",
    "        print(f\"p(δ=0)        : {prior_density_at_zero:.4g}\")\n",
    "        print(f\"p(δ=0 | data) : {posterior_density_at_zero:.4g}\")\n",
    "        print(f\"Evidence for no difference (H₀): \"\n",
    "            f\"BF₀₁ = {BF_01:.4g}  ({label})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0865ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metric = 'val_IoU'\n",
    "\n",
    "unet_macs_best = get_best_metric(unet_macs, unet_macs_metric_lookup, full_metric)\n",
    "swin_macs_best = get_best_metric(swin_macs, swin_macs_metric_lookup, full_metric)\n",
    "\n",
    "unet_ps_best = get_best_metric(unet_ps, unet_ps_metric_lookup, full_metric)\n",
    "print(unet_macs_best)\n",
    "\n",
    "plt.boxplot([unet_macs_best, swin_macs_best, unet_ps_best], tick_labels=['UNet-Macs', 'SwinUnet-Macs', 'UNet_PS'], showfliers=True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0331e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabs     = []      # the Output widgets (one per metric)\n",
    "tab_titles = []    # used to label the tab headers\n",
    "\n",
    "for metric in metrics:\n",
    "\n",
    "    full_metric = 'val_' + metric if not metric.startswith('val_') else metric\n",
    "    out = widgets.Output()          # each metric gets its own Output “sandbox”\n",
    "\n",
    "    with out:                       # everything inside goes only to this tab\n",
    "        clear_output(wait=True)     # keeps the tab clean on reruns\n",
    "\n",
    "        print(f\"### Processing metric: {full_metric}\\n\")\n",
    "\n",
    "        unet_macs_best = get_best_metric(unet_macs, unet_macs_metric_lookup, full_metric)\n",
    "        swin_macs_best = get_best_metric(swin_macs, swin_macs_metric_lookup, full_metric)\n",
    "\n",
    "        #plt.boxplot([unet_best, swin_best], labels=['UNet', 'Swin Transformer'], showfliers=True)\n",
    "\n",
    "\n",
    "\n",
    "        BEST(\n",
    "            pd.concat([\n",
    "                pd.DataFrame({full_metric: unet_best, 'group': 'U-Net | Aerial'}),\n",
    "                pd.DataFrame({full_metric: swin_best, 'group': 'Swin U-Net | Aerial'}),\n",
    "            ]).reset_index(drop=True),\n",
    "            unet_best, swin_best, full_metric,\n",
    "            plot=True                                    \n",
    "        )\n",
    "\n",
    "    # keep references so we can build the Tab afterwards\n",
    "    tabs.append(out)\n",
    "    tab_titles.append(full_metric)\n",
    "\n",
    "tab_widget = widgets.Tab(children=tabs)\n",
    "\n",
    "for i, title in enumerate(tab_titles):\n",
    "    tab_widget.set_title(i, title)   # label each tab\n",
    "\n",
    "display(tab_widget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93d7704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "def filter_and_remap_splits(folder_src: str,\n",
    "                            folder_dst: str,\n",
    "                            split_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    • Walk through *every file* in `folder_src` (sorted for determinism).\n",
    "    • If that file also exists in `folder_dst`, look up:\n",
    "        – its index in the *source* folder\n",
    "        – the split group (train / test / val) that index belongs to\n",
    "        – its index in the *destination* folder\n",
    "    • Append the destination index to a new list under the same split group.\n",
    "    • Any file missing from `folder_dst` is silently skipped.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict  –  same keys as `split_dict`, but containing only the\n",
    "             files that still exist in both folders, with indices\n",
    "             now referring to `folder_dst`.\n",
    "    \"\"\"\n",
    "\n",
    "   \n",
    "    # Sorted listings give stable, reproducible indices\n",
    "    src_files = sorted(os.listdir(folder_src))\n",
    "    dst_files = sorted(os.listdir(folder_dst))\n",
    "\n",
    "    # Fast filename ➜ index lookup for folder_dst\n",
    "    dst_index_of = {name: i for i, name in enumerate(dst_files)}\n",
    "\n",
    "    # Prepare the output dict with the same split keys\n",
    "    updated = {k: [] for k in split_dict}\n",
    "\n",
    "    for group, idx_list in split_dict.items():\n",
    "        for idx in idx_list:\n",
    "            if idx >= len(src_files):\n",
    "                raise IndexError(\n",
    "                    f\"Index {idx} is out of range for source folder \"\n",
    "                    f\"({len(src_files)} files).\"\n",
    "                )\n",
    "            fname = src_files[idx]            # step 1\n",
    "            dst_idx = dst_index_of.get(fname) # step 2\n",
    "            if dst_idx is not None:\n",
    "                updated[group].append(dst_idx)  # step 3\n",
    "\n",
    "    return updated\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Example usage\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ── folders ────────────────────────────────────────────────────────────\n",
    "    FOLDER_A = r\"N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\training_data\\MACS\\20250429-1208_MACS_test_utm8\"   # indices in the JSON point here\n",
    "    FOLDER_B = r\"N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\training_data\\PS\\20250604-0816_Unet_Planet_utm8\"   # new target folder\n",
    "\n",
    "    # ── your JSON split definition (load from file or paste) ───────────────\n",
    "    SPLITS_JSON = \"\"\"\n",
    "    {\n",
    "    \"training_frames\": [550, 28, 374, 15, 134, 462, 193, 112, 544, 341, 582, 513, 385, 24, 263, 347, 452, 249, 6, 623, 285, 11, 195, 105, 632, 608, 237, 570, 315, 143, 220, 475, 640, 625, 432, 149, 226, 402, 202, 451, 620, 255, 499, 288, 401, 246, 369, 360, 334, 345, 595, 316, 319, 168, 225, 581, 245, 63, 536, 624, 508, 503, 540, 408, 291, 426, 96, 657, 434, 375, 86, 478, 548, 524, 419, 188, 151, 37, 217, 328, 596, 407, 161, 532, 250, 318, 652, 42, 353, 39, 472, 384, 313, 493, 569, 448, 92, 70, 101, 5, 382, 3, 630, 209, 443, 234, 206, 546, 45, 644, 290, 79, 163, 264, 575, 517, 441, 587, 424, 303, 157, 591, 122, 450, 329, 572, 147, 456, 376, 483, 178, 592, 423, 377, 387, 230, 605, 236, 551, 491, 571, 75, 85, 626, 396, 616, 260, 169, 277, 185, 446, 431, 390, 90, 258, 467, 46, 340, 265, 281, 110, 29, 633, 386, 504, 490, 152, 357, 509, 205, 336, 476, 227, 97, 22, 647, 177, 515, 166, 114, 266, 180, 184, 604, 395, 354, 651, 398, 123, 567, 106, 634, 203, 58, 153, 447, 656, 321, 663, 496, 454, 224, 461, 4, 218, 528, 299, 19, 140, 179, 103, 223, 181, 113, 137, 530, 615, 573, 599, 421, 57, 568, 404, 389, 327, 50, 425, 267, 350, 485, 74, 397, 391, 338, 502, 403, 415, 579, 331, 221, 138, 637, 470, 642, 361, 229, 172, 409, 459, 55, 576, 111, 636, 535, 139, 622, 62, 135, 9, 132, 469, 545, 482, 487, 199, 593, 65, 192, 339, 500, 588, 17, 141, 44, 323, 435, 413, 439, 521, 284, 533, 175, 484, 658, 427, 643, 73, 486, 272, 349, 366, 373, 510, 531, 2, 1, 607, 216, 378, 631, 578, 242, 333, 165, 159, 392, 543, 549, 405, 312, 231, 370, 279, 102, 442, 116, 198, 38, 298, 52, 89, 617, 465, 81, 358, 522, 213, 26, 99, 562, 317, 553, 297, 614, 32, 489, 586, 668, 8, 94, 48, 160, 274, 14, 296, 16, 628, 367, 606, 326, 268, 144, 598, 121, 259, 474, 344, 270, 444, 372, 66, 660, 77, 83, 87, 262, 278, 10, 602, 257, 43, 93, 308, 563, 594, 523, 302, 115, 537, 650, 449, 239, 453, 21, 330, 519, 207, 34, 210, 150, 35, 301, 211, 31, 0, 219, 240, 414, 359, 639, 68, 351, 468, 190, 394, 365, 649, 379, 171, 148, 648, 269, 458, 305, 182, 618, 20, 584, 64, 556, 547, 53, 343, 12, 346, 154, 383, 56], \n",
    "    \"testing_frames\": [363, 36, 506, 557, 534, 455, 322, 155, 129, 589, 645, 238, 82, 654, 481, 418, 538, 183, 337, 311, 512, 310, 60, 107, 612, 514, 635, 276, 108, 186, 156, 201, 67, 80, 309, 460, 411, 295, 558, 662, 23, 282, 488, 664, 494, 388, 125, 477, 164, 516, 580, 69, 133, 552, 54, 286, 554, 638, 585, 368, 59, 320, 495, 27, 248, 241, 98, 560, 212, 146, 197, 208, 583, 564, 293, 518, 243, 566, 542, 232, 30, 300, 275, 173, 406, 306, 646, 204, 41, 287, 214, 501, 256, 422, 601, 292, 84, 565, 127, 136, 215, 466, 273, 420, 176, 457, 665, 667, 91, 235, 49, 539, 289, 283, 417, 603, 440, 142, 100, 429, 170, 352, 600, 348, 233, 124, 399, 117, 72, 381, 362, 520, 194, 597], \n",
    "    \"validation_frames\": [438, 78, 613, 332, 511, 244, 464, 471, 393, 167, 128, 661, 356, 355, 410, 364, 88, 47, 247, 109, 187, 611, 497, 436, 251, 574, 555, 619, 254, 40, 473, 335, 228, 126, 95, 119, 314, 33, 641, 61, 609, 527, 200, 191, 7, 412, 145, 621, 158, 463, 561, 610, 196, 222, 400, 653, 480, 120, 445, 130, 577, 380, 629, 304, 294, 498, 479, 430, 526, 590, 559, 51, 71, 325, 261, 13, 307, 252, 342, 162, 131, 18, 25, 416, 627, 428, 666, 507, 505, 529, 371, 118, 492, 541, 271, 659, 433, 324, 104, 437, 525, 189, 655, 76, 280, 253, 174]\n",
    "    }\n",
    "    \"\"\"\n",
    "    splits = json.loads(SPLITS_JSON)\n",
    "\n",
    "    # ── remap & show the result ────────────────────────────────────────────\n",
    "    updated = filter_and_remap_splits(FOLDER_A, FOLDER_B, splits)\n",
    "    print(json.dumps(updated, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a80b8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bayesian_anova(df, metric, lower_is_better=False,\n",
    "                       robust=True, hierarchical=True,\n",
    "                       tune=1000, draws=2000, chains=4, target_accept=0.95):\n",
    "    \"\"\"\n",
    "    df must have columns;  metric (float)  and  \"group\" (categorical/string)\n",
    "    \"\"\"\n",
    "    y      = df[metric].values\n",
    "    groups = df[\"group\"].astype(\"category\")\n",
    "    g_idx  = groups.cat.codes.values          # 0 … K-1\n",
    "    K      = int(g_idx.max() + 1)\n",
    "\n",
    "    mu_m, mu_s        = y.mean(), y.std()*2\n",
    "    sigma_low, sigma_high = 1e-1, 10\n",
    "\n",
    "    with pm.Model() as m:\n",
    "        # --- priors on the K group means -----------------------------------\n",
    "        if hierarchical:          # partial-pooling variant\n",
    "            mu_grand = pm.Normal(\"mu_grand\", mu=mu_m, sigma=mu_s)\n",
    "            tau       = pm.HalfNormal(\"tau\", sigma=mu_s)\n",
    "            mu        = pm.Normal(\"mu\", mu=mu_grand, sigma=tau, shape=K)\n",
    "        else:                     # independent means\n",
    "            mu = pm.Normal(\"mu\", mu=mu_m,  sigma=mu_s, shape=K)\n",
    "\n",
    "        # --- priors on the K group stds ------------------------------------\n",
    "        sigma = pm.Uniform(\"sigma\", lower=sigma_low, upper=sigma_high, shape=K)\n",
    "\n",
    "        # --- optional heavy-tailed likelihood ------------------------------\n",
    "        nu = (pm.Exponential(\"nu_minus_1\", 1/29) + 1) if robust else np.inf\n",
    "\n",
    "        pm.StudentT(\"obs\", mu=mu[g_idx], sigma=sigma[g_idx], nu=nu, observed=y)\n",
    "\n",
    "        idata = pm.sample(tune=tune, draws=draws, chains=chains, target_accept=target_accept,\n",
    "                          return_inferencedata=True)\n",
    "\n",
    "    idata.attrs[\"metric\"]           = metric\n",
    "    idata.attrs[\"lower_is_better\"]  = lower_is_better\n",
    "    idata.attrs[\"groups\"]           = groups.cat.categories.tolist()\n",
    "    return idata\n",
    "\n",
    "def prob_each_is_best(idata):\n",
    "    \"\"\"\n",
    "    Return a Series whose index is group names and whose values are the\n",
    "    posterior probabilities that each group has the best (min / max) mean.\n",
    "    Works without the 'bottleneck' package.\n",
    "    \"\"\"\n",
    "    metric_is_lower = idata.attrs[\"lower_is_better\"]\n",
    "    means = (\n",
    "        idata.posterior[\"mu\"]\n",
    "        .stack(sample=(\"chain\", \"draw\"))\n",
    "        .values                              # numpy (K, N)\n",
    "    )\n",
    "\n",
    "    # index of best group in every posterior draw ---------------------------\n",
    "    best_idx = means.argmin(axis=0) if metric_is_lower else means.argmax(axis=0)\n",
    "\n",
    "    K   = means.shape[0]\n",
    "    p   = np.bincount(best_idx, minlength=K) / best_idx.size  # length-K vector\n",
    "    return pd.Series(p, index=idata.attrs[\"groups\"], name=\"p(best)\")\n",
    "\n",
    "\n",
    "def pairwise_contrasts(idata, rope=None):\n",
    "    means = (idata.posterior[\"mu\"]\n",
    "             .stack(sample=(\"chain\", \"draw\"))\n",
    "             .values)            # -> plain (K, N) ndarray\n",
    "    K, N = means.shape\n",
    "    names = idata.attrs[\"groups\"]\n",
    "    rows = []\n",
    "\n",
    "    for i in range(K - 1):\n",
    "        for j in range(i + 1, K):\n",
    "            diff = means[i] - means[j]                # 1-D NumPy\n",
    "            hdi_low, hdi_high = np.quantile(diff, [0.025, 0.975])\n",
    "            prob_gt0 = (diff > 0).mean()\n",
    "\n",
    "            row = dict(A=names[i], B=names[j],\n",
    "                       mean_diff=diff.mean(),\n",
    "                       hdi_low=hdi_low, hdi_high=hdi_high,\n",
    "                       p_A_gt_B=prob_gt0)\n",
    "\n",
    "            if rope is not None:\n",
    "                row[\"p_in_rope\"] = ((rope[0] < diff) & (diff < rope[1])).mean()\n",
    "            rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96bad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "group_data = {\n",
    "    \"U-Net | Aerial\": (unet_macs, unet_macs_metric_lookup),\n",
    "    \"U-Net | PS\" : (unet_ps,   unet_ps_metric_lookup),\n",
    "    \"Swin U-Net | Aerial\": (swin_macs,  swin_macs_metric_lookup),\n",
    "    \"Swin U-Net | PS\": (swin_ps, swin_ps_metric_lookup),\n",
    "           # add more …\n",
    "    # \"rest\": (rest_data_array,  rest_metric_lookup),\n",
    "}\n",
    "\n",
    "\n",
    "minimal_metrics = {\n",
    "    \"val_loss\", \"loss\",\n",
    "    \"val_Hausdorff_distance\", \"Hausdorff_distance\",\n",
    "}\n",
    "\n",
    "# convenience ----------------------------------------------------------------\n",
    "def best_per_group(metric_name):\n",
    "    \"\"\"Return a dict {group_name: 1-D array of best values (one per run)}.\"\"\"\n",
    "    out = {}\n",
    "    for g, (arr, lookup) in group_data.items():\n",
    "        out[g] = get_best_metric(arr, lookup, metric_name)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  One tab per metric\n",
    "# ------------------------------------------------------------------\n",
    "tabs, tab_titles = [], []\n",
    "\n",
    "for metric in metrics:\n",
    "    full_metric = (\n",
    "        \"val_\" + metric \n",
    "    )\n",
    "    box = widgets.Output()                 # \n",
    "\n",
    "    with box:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"### Processing metric: {full_metric}\\n\")\n",
    "\n",
    "        # --- gather the “best epoch” values for all groups -----------\n",
    "        best = best_per_group(full_metric)           # dict of 1-D arrays\n",
    "\n",
    "        # --- quick exploratory box-plot ------------------------------\n",
    "        plt.boxplot(best.values(), tick_labels=best.keys(), showfliers=True)\n",
    "        plt.title(full_metric)\n",
    "        plt.show()\n",
    "\n",
    "        # --- tidy dataframe for the model ----------------------------\n",
    "        df = pd.concat(\n",
    "            [pd.DataFrame({full_metric: v, \"group\": k})\n",
    "             for k, v in best.items()],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "        print(df)\n",
    "\n",
    "        # --- run the Bayesian ANOVA model ----------------------------\n",
    "        idata = fit_bayesian_anova(\n",
    "            df, full_metric,\n",
    "            lower_is_better=full_metric in minimal_metrics,\n",
    "            hierarchical=True,            # or False if you prefer\n",
    "            robust=True,\n",
    "        )\n",
    "\n",
    "        # --- 2a.   Which group is most likely best? ------------------\n",
    "        print(\"\\nPosterior probability each group is the best:\")\n",
    "        display(prob_each_is_best(idata).sort_values(ascending=False))\n",
    "\n",
    "        # --- 2b.   Credible differences / Bayes factors --------------\n",
    "        print(\"\\nPair-wise contrasts (95 % HDI and p):\")\n",
    "        display(pairwise_contrasts(idata, rope=[-0.005, 0.005]))\n",
    "\n",
    "        # --- optional visuals ----------------------------------------\n",
    "        az.plot_forest(idata, var_names=\"mu\")\n",
    "        plt.title(f\"{full_metric} – group means with 95 % HDI\")\n",
    "        plt.show()\n",
    "\n",
    "        az.plot_posterior(idata, var_names=\"mu\", ref_val=0)\n",
    "        plt.title(f\"{full_metric} – posteripr for group means with 95 % HDI\")\n",
    "        plt.show()\n",
    "\n",
    "    # keep handles so we can build the tab view afterwards\n",
    "    tabs.append(box)\n",
    "    tab_titles.append(full_metric)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  Assemble the interactive widget\n",
    "# ------------------------------------------------------------------\n",
    "tab_widget = widgets.Tab(children=tabs)\n",
    "for i, title in enumerate(tab_titles):\n",
    "    tab_widget.set_title(i, title)\n",
    "display(tab_widget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301b803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = r'/isipd/projects/p_planetdw/data/methods_test/logs/unet_tuning/20251024-113444_unet_tuning_HB_best_hparams.json'\n",
    "\n",
    "open(path):\n",
    "    file = json(path)\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f838b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3D flight snapshots — UTM 4N, point-cloud (RGB+DEM) ground, VE, trail colored by altitude ===\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
    "import fiona\n",
    "from pyproj import CRS, Transformer\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.warp import calculate_default_transform, reproject\n",
    "import math\n",
    "\n",
    "# --------------------------- knobs ---------------------------\n",
    "VERT_EXAG = 5         # vertical exaggeration (>=1)\n",
    "VIEW_ELEV = 28           # camera elevation (deg)\n",
    "VIEW_AZIM = -100         # camera azimuth (deg)\n",
    "DRAW_NORTH_ARROW = True  # overlay a 2D north arrow\n",
    "SNAPSHOT_FREQ = \"30S\"     # frame cadence\n",
    "OUT_DPI = 500            # PNG dpi\n",
    "DEBUG_PRINT = False      # set True to print shapes/ranges once\n",
    "\n",
    "# point cloud sampling\n",
    "PC_TARGET_RES_M = 1.0    # reproject rasters to this pixel size (m)\n",
    "PC_STEP = 5              # sample every N pixels (1 => every meter)\n",
    "PC_MAX_POINTS = 2_000_000\n",
    "PC_MARKER_SIZE = 0.1     # scatter size (points)\n",
    "\n",
    "EPSG_UTM4N = 32604       # UTM zone 4N (meters)\n",
    "\n",
    "# --------------------------- IO paths ---------------------------\n",
    "GPKG_PATH = r\"P:/cstadie/track_selawik2.gpkg\"\n",
    "IMAGE_PATH = r\"N:\\response\\RS_imagery\\MACS\\PermaX_2024\\data_products\\WA_SelawikSlump_20240711_15cm_01\\WA_SelawikSlump_20240711_15cm_01_Ortho.tif\"   # RGB/RGBA GeoTIFF\n",
    "DEM_PATH   = r\"N:\\response\\RS_imagery\\MACS\\PermaX_2024\\data_products\\WA_SelawikSlump_20240711_15cm_01\\WA_SelawikSlump_20240711_15cm_01_DSM.tif\"     # DEM GeoTIFF\n",
    "OUT_DIR    = r\"P:/cstadie/snapshots_30s_pc\"\n",
    "\n",
    "# --------------------------- helpers ---------------------------\n",
    "def read_track_with_fiona(gpkg_path: str | Path, layer: str | None = None,\n",
    "                          time_format: str | None = None) -> pd.DataFrame:\n",
    "    \"\"\"Read a GPKG with Fiona, extract Point coords + fields to a DataFrame (UTM4N).\"\"\"\n",
    "    gpkg_path = Path(gpkg_path)\n",
    "    rows = []\n",
    "    with fiona.open(gpkg_path, layer=layer) as src:\n",
    "        if src.crs_wkt:\n",
    "            src_crs = CRS.from_wkt(src.crs_wkt)\n",
    "        elif src.crs:\n",
    "            src_crs = CRS.from_user_input(src.crs)\n",
    "        else:\n",
    "            src_crs = CRS.from_epsg(4326)\n",
    "\n",
    "        to_utm = Transformer.from_crs(src_crs, CRS.from_epsg(EPSG_UTM4N), always_xy=True)\n",
    "        to_wgs84 = Transformer.from_crs(src_crs, CRS.from_epsg(4326), always_xy=True)\n",
    "\n",
    "        for feat in src:\n",
    "            geom = feat.get(\"geometry\")\n",
    "            if not geom or geom.get(\"type\") != \"Point\":\n",
    "                continue\n",
    "            lon_src, lat_src = map(float, geom[\"coordinates\"][:2])\n",
    "            lon, lat = to_wgs84.transform(lon_src, lat_src)\n",
    "            x_utm, y_utm = to_utm.transform(lon_src, lat_src)\n",
    "\n",
    "            p = feat.get(\"properties\", {}) or {}\n",
    "            rows.append({\n",
    "                \"lon\": lon, \"lat\": lat,\n",
    "                \"x_utm\": x_utm, \"y_utm\": y_utm,\n",
    "                \"UTCTime\": p.get(\"UTCTime\"),\n",
    "                \"H-MSL\": p.get(\"H-MSL\"),\n",
    "                \"Roll\": p.get(\"Roll\"),\n",
    "                \"Stati\": p.get(\"Stati\"),\n",
    "            })\n",
    "\n",
    "    if not rows:\n",
    "        raise ValueError(\"No Point features found in the specified layer/file.\")\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # time -> datetime64[ns]\n",
    "    if time_format:\n",
    "        df[\"UTCTime\"] = pd.to_datetime(df[\"UTCTime\"], format=time_format, errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"UTCTime\"] = pd.to_datetime(df[\"UTCTime\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"UTCTime\"]).reset_index(drop=True)\n",
    "    df[\"UTCTime\"] = df[\"UTCTime\"].astype(\"datetime64[ns]\")\n",
    "\n",
    "    # numeric fields\n",
    "    for col in [\"H-MSL\", \"Roll\", \"Stati\"]:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # chronological order (stable)\n",
    "    df = df.sort_values([\"UTCTime\", \"Stati\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_segments(df: pd.DataFrame):\n",
    "    \"\"\"Build 3D flight-line segments in UTM4N, breaking at 'Stati' gaps (no bridging).\"\"\"\n",
    "    x = df[\"x_utm\"].to_numpy(float)\n",
    "    y = df[\"y_utm\"].to_numpy(float)\n",
    "    z = df[\"H-MSL\"].to_numpy(float)\n",
    "    stati = df[\"Stati\"].to_numpy(float)\n",
    "\n",
    "    stati_diff = np.empty_like(stati); stati_diff[:] = np.nan\n",
    "    stati_diff[1:] = stati[1:] - stati[:-1]\n",
    "    gap_start = (np.isnan(stati_diff)) | (stati_diff != 1)\n",
    "\n",
    "    x_plot, y_plot, z_plot = x.copy(), y.copy(), z.copy()\n",
    "    x_plot[gap_start] = np.nan\n",
    "    y_plot[gap_start] = np.nan\n",
    "    z_plot[gap_start] = np.nan\n",
    "\n",
    "    valid = ~np.isnan(x_plot) & ~np.isnan(y_plot) & ~np.isnan(z_plot)\n",
    "    connect = valid & np.roll(valid, 1); connect[0] = False\n",
    "\n",
    "    seg_starts = np.where(connect)[0] - 1\n",
    "    seg_ends = np.where(connect)[0]\n",
    "    if seg_ends.size == 0:\n",
    "        raise ValueError(\"No drawable segments found (after gap handling).\")\n",
    "\n",
    "    segments = np.stack(\n",
    "        [\n",
    "            np.stack([x_plot[seg_starts], y_plot[seg_starts], z_plot[seg_starts]], axis=1),\n",
    "            np.stack([x_plot[seg_ends],   y_plot[seg_ends],   z_plot[seg_ends]],   axis=1),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    return segments, seg_ends\n",
    "\n",
    "\n",
    "# ---------- point-cloud (RGB + DEM) sampling in UTM4N ----------\n",
    "def _reproject_to(crs_dst: CRS, src_ds: rasterio.io.DatasetReader, res: float, resampling=Resampling.bilinear):\n",
    "    \"\"\"Reproject a raster to crs_dst at given pixel size; return (array, transform).\"\"\"\n",
    "    transform, width, height = calculate_default_transform(\n",
    "        src_ds.crs, crs_dst, src_ds.width, src_ds.height, *src_ds.bounds, resolution=res\n",
    "    )\n",
    "    out = np.zeros((src_ds.count, height, width), dtype=src_ds.dtypes[0])\n",
    "    reproject(\n",
    "        source=rasterio.band(src_ds, list(range(1, src_ds.count + 1))),\n",
    "        destination=out,\n",
    "        src_transform=src_ds.transform, src_crs=src_ds.crs,\n",
    "        dst_transform=transform,       dst_crs=crs_dst,\n",
    "        resampling=resampling,\n",
    "        src_nodata=src_ds.nodata, dst_nodata=src_ds.nodata,\n",
    "        num_threads=2,\n",
    "    )\n",
    "    return out, transform\n",
    "\n",
    "def _overlap_windows(transform_a, shape_a, transform_b, shape_b):\n",
    "    \"\"\"Overlap windows (rows, cols) for two rasters in the same CRS.\"\"\"\n",
    "    def bounds(transform, shape):\n",
    "        h, w = shape\n",
    "        x0, y0 = transform * (0, 0)\n",
    "        x1, y1 = transform * (w, h)\n",
    "        xmin, xmax = sorted([x0, x1])\n",
    "        ymin, ymax = sorted([y0, y1])\n",
    "        return xmin, ymin, xmax, ymax\n",
    "\n",
    "    axmin, aymin, axmax, aymax = bounds(transform_a, shape_a)\n",
    "    bxmin, bymin, bxmax, bymax = bounds(transform_b, shape_b)\n",
    "\n",
    "    ixmin = max(axmin, bxmin); iymin = max(aymin, bymin)\n",
    "    ixmax = min(axmax, bxmax); iymax = min(aymax, bymax)\n",
    "    if not (ixmin < ixmax and iymin < iymax):\n",
    "        return None\n",
    "\n",
    "    def window(transform, shape, xmin, ymin, xmax, ymax):\n",
    "        inv = ~transform\n",
    "        c0, r0 = inv * (xmin, ymax)  # top-left\n",
    "        c1, r1 = inv * (xmax, ymin)  # bottom-right\n",
    "        r0i = max(0, int(math.floor(r0))); c0i = max(0, int(math.floor(c0)))\n",
    "        r1i = min(shape[0], int(math.ceil(r1))); c1i = min(shape[1], int(math.ceil(c1)))\n",
    "        return r0i, c0i, max(0, r1i - r0i), max(0, c1i - c0i)\n",
    "\n",
    "    win_a = window(transform_a, shape_a, ixmin, iymin, ixmax, iymax)\n",
    "    win_b = window(transform_b, shape_b, ixmin, iymin, ixmax, iymax)\n",
    "    return win_a, win_b\n",
    "\n",
    "def _grid_xy(transform, rows, cols, step_px=1):\n",
    "    \"\"\"XY arrays (projected units) of pixel centers over a window, sampled every step_px.\"\"\"\n",
    "    xs = (np.arange(0, cols, step_px) + 0.5)\n",
    "    ys = (np.arange(0, rows, step_px) + 0.5)\n",
    "    C, R = np.meshgrid(xs, ys)\n",
    "    X = transform.c + C * transform.a + R * transform.b\n",
    "    Y = transform.f + C * transform.d + R * transform.e\n",
    "    return X, Y\n",
    "\n",
    "def sample_pointcloud_rgb_dem(\n",
    "    image_path,\n",
    "    dem_path,\n",
    "    target_epsg=EPSG_UTM4N,\n",
    "    target_res=PC_TARGET_RES_M,\n",
    "    step=PC_STEP,\n",
    "    max_points=PC_MAX_POINTS,\n",
    "    stretch_pct=(2, 98),   # robust contrast stretch\n",
    "    gamma=1.0,             # e.g., 1.6–2.2 to brighten; 1.0 = no gamma\n",
    "    swap_bgr=False,        # set True if your TIFF bands are B,G,R instead of R,G,B\n",
    "):\n",
    "    \"\"\"Return (X, Y, Z, RGB in 0..1) in UTM4N from the RGB/DEM overlap.\"\"\"\n",
    "    crs_dst = CRS.from_epsg(target_epsg)\n",
    "\n",
    "    with rasterio.open(image_path) as rgb_ds, rasterio.open(dem_path) as dem_ds:\n",
    "        rgb_dtype = np.dtype(rgb_ds.dtypes[0])  # remember for scaling\n",
    "        rgb_arr, rgb_tf = _reproject_to(crs_dst, rgb_ds, res=target_res, resampling=Resampling.bilinear)\n",
    "        dem_arr, dem_tf = _reproject_to(crs_dst, dem_ds, res=target_res, resampling=Resampling.bilinear)\n",
    "\n",
    "    # bands (ignore alpha if present)\n",
    "    if rgb_arr.shape[0] < 3:\n",
    "        raise ValueError(\"RGB image must have at least 3 bands.\")\n",
    "    B, G, R = rgb_arr[0], rgb_arr[1], rgb_arr[2]\n",
    "    if swap_bgr:\n",
    "        R, G, B = B, G, R\n",
    "\n",
    "    DEM = dem_arr[0] if dem_arr.ndim == 3 else dem_arr\n",
    "\n",
    "    # overlap window\n",
    "    wins = _overlap_windows(rgb_tf, R.shape, dem_tf, DEM.shape)\n",
    "    if wins is None:\n",
    "        raise RuntimeError(\"No spatial overlap between RGB and DEM (after reprojection).\")\n",
    "    (r0_rgb, c0_rgb, rows, cols), (r0_dem, c0_dem, rows2, cols2) = wins\n",
    "    rows, cols = min(rows, rows2), min(cols, cols2)\n",
    "\n",
    "    # crop overlap\n",
    "    Rw = R[r0_rgb:r0_rgb+rows, c0_rgb:c0_rgb+cols]\n",
    "    Gw = G[r0_rgb:r0_rgb+rows, c0_rgb:c0_rgb+cols]\n",
    "    Bw = B[r0_rgb:r0_rgb+rows, c0_rgb:c0_rgb+cols]\n",
    "    Zw = DEM[r0_dem:r0_dem+rows, c0_dem:c0_dem+cols]\n",
    "\n",
    "    # subsample + coords\n",
    "    Rw, Gw, Bw, Zw = Rw[::step, ::step], Gw[::step, ::step], Bw[::step, ::step], Zw[::step, ::step]\n",
    "    tf_win = rgb_tf * rasterio.Affine.translation(c0_rgb, r0_rgb)\n",
    "    Xg, Yg = _grid_xy(tf_win, rows, cols, step_px=step)\n",
    "\n",
    "    # mask valid\n",
    "    mask = np.isfinite(Zw) & np.isfinite(Rw) & np.isfinite(Gw) & np.isfinite(Bw)\n",
    "\n",
    "    X, Y, Z = Xg[mask], Yg[mask], Zw[mask]\n",
    "\n",
    "    # ---- normalize by dtype scale ----\n",
    "    if np.issubdtype(rgb_dtype, np.uint8):\n",
    "        base_scale = 255.0\n",
    "    elif np.issubdtype(rgb_dtype, np.uint16):\n",
    "        base_scale = 65535.0\n",
    "    else:\n",
    "        base_scale = 1.0  # float imagery often already 0..1\n",
    "\n",
    "    rgb_stack = np.stack([Rw[mask], Gw[mask], Bw[mask]], axis=1).astype(np.float32) / base_scale\n",
    "\n",
    "    # ---- per-band robust stretch to 0..1 ----\n",
    "    if stretch_pct is not None:\n",
    "        lo_p, hi_p = stretch_pct\n",
    "        for b in range(3):\n",
    "            ch = rgb_stack[:, b]\n",
    "            lo = np.percentile(ch, lo_p)\n",
    "            hi = np.percentile(ch, hi_p)\n",
    "            if hi <= lo:   # degenerate case\n",
    "                hi, lo = ch.max(), ch.min()\n",
    "            ch = (ch - lo) / max(hi - lo, 1e-6)\n",
    "            rgb_stack[:, b] = np.clip(ch, 0.0, 1.0)\n",
    "\n",
    "    # ---- optional gamma correction (brighten mid-tones) ----\n",
    "    if gamma and gamma != 1.0:\n",
    "        rgb_stack = np.clip(rgb_stack, 0.0, 1.0) ** (1.0 / float(gamma))\n",
    "\n",
    "    # thin if too many points\n",
    "    if X.size > max_points:\n",
    "        idx = np.random.choice(X.size, int(max_points), replace=False)\n",
    "        X, Y, Z, rgb_stack = X[idx], Y[idx], Z[idx], rgb_stack[idx]\n",
    "\n",
    "    if DEBUG_PRINT:\n",
    "        print(f\"[PC] points: {X.size:,}  RGB range per band:\",\n",
    "              [f\"{rgb_stack[:,i].min():.3f}-{rgb_stack[:,i].max():.3f}\" for i in range(3)],\n",
    "              f\"  Z: {Z.min():.2f}-{Z.max():.2f}\")\n",
    "\n",
    "    return X, Y, Z, rgb_stack\n",
    "\n",
    "\n",
    "\n",
    "def save_snapshots_every_30s_with_pointcloud(\n",
    "    df: pd.DataFrame,\n",
    "    image_path: str | Path,\n",
    "    dem_path: str | Path,\n",
    "    out_dir: str | Path = OUT_DIR,\n",
    "    dpi: int = OUT_DPI,\n",
    "):\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ---- precompute flight segments & color by ALTITUDE ----\n",
    "    segments, seg_ends = build_segments(df)\n",
    "    alt = df[\"H-MSL\"].to_numpy(float)\n",
    "    seg_val = alt[seg_ends]\n",
    "    cmap = plt.cm.magma\n",
    "    vmin, vmax = np.nanmin(seg_val), np.nanmax(seg_val)\n",
    "    if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin == vmax:\n",
    "        vmin, vmax = vmin - 0.5, vmin + 0.5\n",
    "\n",
    "    # ---- sample point cloud ONCE (UTM4N) ----\n",
    "    Xpc, Ypc, Zpc, Cpc = sample_pointcloud_rgb_dem(image_path, dem_path)\n",
    "\n",
    "    # ---- limits combine track and point cloud ----\n",
    "    x_all = np.concatenate([df[\"x_utm\"].to_numpy(float), Xpc])\n",
    "    y_all = np.concatenate([df[\"y_utm\"].to_numpy(float), Ypc])\n",
    "    z_all = np.concatenate([df[\"H-MSL\"].to_numpy(float), Zpc])\n",
    "\n",
    "    pad_m = 200\n",
    "    xmin, xmax = np.nanmin(x_all) - pad_m, np.nanmax(x_all) + pad_m\n",
    "    ymin, ymax = np.nanmin(y_all) - pad_m, np.nanmax(y_all) + pad_m\n",
    "    zmin, zmax = 0.0, np.nanmax(z_all) + 20.0\n",
    "\n",
    "    dx, dy, dz = (xmax - xmin), (ymax - ymin), (zmax - zmin)\n",
    "    ax_aspect = (dx, dy, dz * VERT_EXAG)\n",
    "\n",
    "    # ---- frame times ----\n",
    "    t0 = df[\"UTCTime\"].min().floor(\"s\")\n",
    "    t1 = df[\"UTCTime\"].max().ceil(\"s\")\n",
    "    times = pd.date_range(t0, t1, freq=SNAPSHOT_FREQ)\n",
    "    seg_end_times = df.loc[seg_ends, \"UTCTime\"].to_numpy(dtype=\"datetime64[ns]\")\n",
    "\n",
    "    # ---- render each snapshot ----\n",
    "    for i, t in enumerate(times):\n",
    "        k = np.searchsorted(seg_end_times, t.to_datetime64(), side=\"right\")\n",
    "        if k == 0:\n",
    "            segs_to_draw = np.empty((0, 2, 3)); vals_to_draw = np.empty((0,))\n",
    "            tip = None\n",
    "        else:\n",
    "            segs_to_draw = segments[:k]\n",
    "            vals_to_draw = seg_val[:k]\n",
    "            tip = segs_to_draw[-1][1]\n",
    "\n",
    "        fig = plt.figure(figsize=(12, 9))\n",
    "        ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "        # pane styling (no solid floor/walls)\n",
    "        for axis in (ax.xaxis, ax.yaxis, ax.zaxis):\n",
    "            try:\n",
    "                axis.pane.fill = False\n",
    "                axis.pane.set_edgecolor((0, 0, 0, 0.25))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # --- point cloud ground (RGB colors) ---\n",
    "        ax.scatter(Xpc, Ypc, Zpc, c=Cpc,marker=',', s=PC_MARKER_SIZE, depthshade=False, zorder=0)\n",
    "\n",
    "        # --- flight trail (colored by altitude) ---\n",
    "        trail = Line3DCollection(segs_to_draw, cmap=cmap, linewidth=2.0, zorder=5)\n",
    "        trail.set_array(vals_to_draw); trail.set_clim(vmin=vmin, vmax=vmax)\n",
    "        ax.add_collection3d(trail, autolim=False)\n",
    "\n",
    "        # current tip\n",
    "        if tip is not None:\n",
    "            ax.scatter([tip[0]], [tip[1]], [tip[2]], marker='>',\n",
    "                       s=50, c=\"white\", edgecolors=\"k\", linewidths=0.6, alpha=0.95, zorder=10)\n",
    "\n",
    "        # colorbar (altitude)\n",
    "        cbar = fig.colorbar(\n",
    "            plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=vmin, vmax=vmax)),\n",
    "            ax=ax, shrink=0.6, pad=0.07\n",
    "        )\n",
    "        cbar.set_label(\"Altitude (m MSL)\")\n",
    "\n",
    "        # axes, limits, labels (hide XY ticks for a cleaner map feel)\n",
    "        ax.set_xlim(xmin, xmax); ax.set_ylim(ymin, ymax); ax.set_zlim(zmin, zmax)\n",
    "        ax.set_xlabel(\"\"); ax.set_ylabel(\"\"); ax.set_zlabel(\"Altitude (m)\")\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "        # vertical exaggeration + camera\n",
    "        ax.set_box_aspect(ax_aspect)\n",
    "        ax.view_init(elev=VIEW_ELEV, azim=VIEW_AZIM)\n",
    "\n",
    "        # light grid\n",
    "        try:\n",
    "            ax.grid(True)\n",
    "            for _ax in (ax.xaxis, ax.yaxis, ax.zaxis):\n",
    "                _ax._axinfo[\"grid\"][\"linewidth\"] = 0.4\n",
    "                _ax._axinfo[\"grid\"][\"color\"] = (0, 0, 0, 0.2)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # north arrow overlay\n",
    "        if DRAW_NORTH_ARROW:\n",
    "            nar = fig.add_axes([0.08, 0.84, 0.04, 0.12])\n",
    "            nar.set_axis_off()\n",
    "            nar.arrow(0.5, 0.1, 0.0, 0.75,\n",
    "                      width=0.03, head_width=0.12, head_length=0.12,\n",
    "                      fc=\"black\", ec=\"black\", length_includes_head=True)\n",
    "            nar.text(0.5, 0.98, \"N\", ha=\"center\", va=\"top\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # save frame\n",
    "        stamp = pd.Timestamp(t).strftime(\"%Y%m%dT%H%M%S\")\n",
    "        out_path = Path(out_dir) / f\"frame_{i:04d}_{stamp}.png\"\n",
    "        fig.savefig(out_path, dpi=dpi)\n",
    "        plt.close(fig)\n",
    "\n",
    "    print(f\"[OK] Wrote {len(times)} snapshots (UTM 4N, point-cloud ground, VE={VERT_EXAG}) to: {Path(out_dir).resolve()}\")\n",
    "\n",
    "\n",
    "# --------------------------- run ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    df = read_track_with_fiona(GPKG_PATH, layer=None, time_format=None)\n",
    "    save_snapshots_every_30s_with_pointcloud(\n",
    "        df,\n",
    "        image_path=IMAGE_PATH,\n",
    "        dem_path=DEM_PATH,\n",
    "        out_dir=OUT_DIR,\n",
    "        dpi=OUT_DPI,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d934d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "image_path = r'N:\\isipd\\projects\\p_planetdw\\data\\dw_detection\\PlanetScope\\images\\2023'\n",
    "aoi_path = r'N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\aois'\n",
    "\n",
    "# create list of all paths of images\n",
    "img_list = [os.path.join(image_path, f) for f in os.listdir(image_path) if f.endswith('.jp2')]\n",
    "aoi_list = [os.path.join(aoi_path, f) for f in os.listdir(aoi_path) if f.endswith(('.gpkg', '.geojson'))]\n",
    "\n",
    "print(f'Found {len(img_list)} images and {len(aoi_list)} aois.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29452fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of which images intersect which aois\n",
    "from shapely.geometry import box\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import tqdm\n",
    "\n",
    "img_aoi_dict = {}\n",
    "\n",
    "for img_path in tqdm.tqdm(img_list):\n",
    "\n",
    "    with rasterio.open(img_path) as src:\n",
    "        bounds = src.bounds\n",
    "        crs = src.crs\n",
    "        bbox = box(bounds.left, bounds.bottom, bounds.right, bounds.top)\n",
    "        img_name = os.path.basename(img_path)\n",
    "\n",
    "        for aoi_path in aoi_list:\n",
    "            aoi = gpd.read_file(aoi_path)\n",
    "            if aoi.crs != crs:\n",
    "                aoi = aoi.to_crs(crs)\n",
    "            \n",
    "            if aoi.intersects(bbox).any():\n",
    "                if img_path not in img_aoi_dict:\n",
    "                    img_aoi_dict[img_path] = []\n",
    "                \n",
    "                img_aoi_dict[img_path].append(aoi_path)\n",
    "                print(f'image {img_name} intersects aoi {os.path.basename(aoi_path)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfc8f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_ortho_tifs(src_dir, dst_dir):\n",
    "    \"\"\"\n",
    "    Recursively scan `src_dir` for .tif files ending with '_Ortho'\n",
    "    and copy them into `dst_dir`.\n",
    "    \"\"\"\n",
    "    # make sure destination exists\n",
    "    os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "    for root, _, files in os.walk(src_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(\".tif\"):\n",
    "                basename, _ = os.path.splitext(file)\n",
    "                if basename.endswith(\"\"):\n",
    "                    src_path = os.path.join(root, file)\n",
    "                    dst_path = os.path.join(dst_dir, file)\n",
    "\n",
    "                    if os.path.exists(dst_path):\n",
    "                        print(f'Skipping {basename}. exists....')\n",
    "                        continue\n",
    "\n",
    "                    # copy file, overwrite if already exists\n",
    "                    shutil.copy2(src_path, dst_path)\n",
    "                    print(f\"Copied: {src_path} → {dst_path}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "dst_folder = r\"N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\training_images\\S2\"\n",
    "src_folder = r\"n:\\isipd\\projects\\p_planetdw\\data\\methods_test\\s2_outputs\"\n",
    "\n",
    "copy_ortho_tifs(src_folder, dst_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3206565",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "total = 1082\n",
    "outside = total-778\n",
    "without = 778-199\n",
    "within = 199\n",
    "\n",
    "check = outside+without+within\n",
    "if check != total:\n",
    "    print(\"Doesn't add up\")\n",
    "\n",
    "else:\n",
    "\n",
    "    data = [outside, without, within]\n",
    "    labels = ['not in AOI', 'no dw', 'with dw']\n",
    "    plt.pie(data, labels=labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92518a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.warp import reproject, Resampling\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- inputs ----\n",
    "r1_path = r\"/path/to/reference.tif\"   # target grid/CRS\n",
    "r2_path = r\"/path/to/other.tif\"       # will be reprojected to r1\n",
    "out_diff = None\n",
    "\n",
    "with rasterio.open(r1_path) as r1, rasterio.open(r2_path) as r2:\n",
    "    a1 = r1.read(1).astype(\"float32\")\n",
    "    a2r = np.full(r1.shape, np.nan, dtype=\"float32\")  # reprojected r2\n",
    "\n",
    "    # nodata to NaN\n",
    "    if r1.nodata is not None:\n",
    "        a1[a1 == r1.nodata] = np.nan\n",
    "\n",
    "    reproject(\n",
    "        source=rasterio.band(r2, 1),\n",
    "        destination=a2r,\n",
    "        src_transform=r2.transform,\n",
    "        src_crs=r2.crs,\n",
    "        src_nodata=r2.nodata,\n",
    "        dst_transform=r1.transform,\n",
    "        dst_crs=r1.crs,\n",
    "        dst_nodata=np.nan,\n",
    "        resampling=Resampling.bilinear,  # use nearest if categorical\n",
    "    )\n",
    "\n",
    "# difference on aligned arrays\n",
    "diff = a1 - a2r\n",
    "\n",
    "# display\n",
    "vmin, vmax = np.nanpercentile(diff, [2, 98])\n",
    "plt.figure()\n",
    "plt.imshow(diff, cmap=\"bwr\", vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label=\"Difference\")\n",
    "plt.title(\"Raster Difference (r1 - reprojected r2)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(diff[~np.isnan(diff)], bins=60)\n",
    "plt.xlabel(\"Difference\"); plt.ylabel(\"Count\"); plt.title(\"Histogram\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "if out_diff:\n",
    "    with rasterio.open(r1_path) as r1:\n",
    "        prof = r1.profile\n",
    "    prof.update(dtype=\"float32\", count=1, nodata=np.nan, compress=\"lzw\")\n",
    "    with rasterio.open(out_diff, \"w\", **prof) as dst:\n",
    "        dst.write(diff, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd6c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/isipd/projects/p_planetdw/data/methods_test/logs/unet_tuning/20251022-133752_unet_tuning_HB_best_hparams.json') as f:\n",
    "    d=json.load(f)\n",
    "    print(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5154c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "dat = pd.read_csv(r\"N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\logs\\20251022-1518_UNKtest_metrics.csv\")\n",
    "display(dat)\n",
    "plt.plot(dat['loss'])\n",
    "\n",
    "metrics = dat.columns\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.plot(dat[f'{metric}'])\n",
    "    plt.title(f'{metric}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ee9639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 27 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying PS scenes: 100%|██████████| 27/27 [00:36<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copied: 27\n",
      "missing: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(r\"\\\\smb.isipd.dmawi.de\\projects\\p_planetdw\\data\\methods_test\\aois\\PSScene_used_selection_summary.csv\")\n",
    "\n",
    "item_ids = df[\"ItemID\"].dropna().astype(str).tolist()\n",
    "print(f\"found {len(item_ids)} items\")\n",
    "\n",
    "basedir = r\"N:\\isipd\\projects\\p_aicore_pf\\initze\\data\\planet\\planet_data_inference_grid\\scenes\"\n",
    "out_dir = r\"N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\training_images\\PS\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# adjust extension if needed\n",
    "EXT = \".tif\"\n",
    "\n",
    "missing = 0\n",
    "copied = 0\n",
    "\n",
    "for item_id in tqdm(item_ids, desc=\"Copying PS scenes\"):\n",
    "    filename = f\"{item_id}_3B_AnalyticMS_SR{EXT}\"\n",
    "    src = os.path.join(basedir, item_id, filename)\n",
    "\n",
    "    if not os.path.isfile(src):\n",
    "        missing += 1\n",
    "        continue\n",
    "\n",
    "    dst = os.path.join(out_dir, filename)\n",
    "    shutil.copy2(src, dst)   # copy2 preserves timestamps/metadata\n",
    "    copied += 1\n",
    "\n",
    "print(f\"copied: {copied}\")\n",
    "print(f\"missing: {missing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b044d027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20250810_211137_60_24ee', '20250721_210140_05_2506', '20250809_210813_01_2532', '20250809_210815_37_2532', '20250727_211540_28_24ed', '20250801_210138_59_2500', '20250802_211323_21_24fe', '20250801_210635_98_24db', '20250802_210848_58_251c', '20250802_210850_82_251c', '20250812_211130_24_24fe', '20250812_211130_24_24fe', '20250716_211521_45_24ed', '20250720_211304_89_2518', '20250720_210807_46_2516', '20250727_211147_24_24dc']\n",
      "16\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "df = pd.read_csv(r\"\\\\smb.isipd.dmawi.de\\projects\\p_planetdw\\data\\methods_test\\aois\\PSScene_used_selection_summary.csv\")\n",
    "folder = r\"N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\training_images\\PS\"\n",
    "\n",
    "new_img_ids = df[\"ItemID\"].astype(str).tolist()\n",
    "\n",
    "imgs = os.listdir(folder)  # or: [p.name for p in Path(folder).iterdir() if p.is_file()]\n",
    "\n",
    "ext_ids_set = set()\n",
    "for img in imgs:\n",
    "    img_id = str(img)[:23]  # keep your rule\n",
    "    ext_ids_set.add(img_id)\n",
    "\n",
    "final_list = [x for x in new_img_ids if x[:23] not in ext_ids_set]\n",
    "\n",
    "print(final_list)\n",
    "print(len(final_list))\n",
    "print(len(new_img_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d6cec33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7222222222222223\n",
      "21700000.0\n"
     ]
    }
   ],
   "source": [
    "dwc = 3100000 #tC\n",
    "mrd = 1800000 #km2\n",
    "\n",
    "dwc_per_km2 = dwc / mrd\n",
    "print(dwc_per_km2)\n",
    "\n",
    "total_catchment_km2 = 12600000  # km2\n",
    "estimated_dwC = total_catchment_km2 * dwc_per_km2 \n",
    "print(estimated_dwC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab76a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed per-folder uncertainties:\n",
      "                checkpoint_id  mean_epistemic_uncertainty_new\n",
      "0  20260117-1411_TERRAMINDxS2                   -2.768802e-10\n",
      "1  20260120-0233_TERRAMINDxS2                   -1.872535e-11\n",
      "2  20260117-0410_TERRAMINDxS2                   -5.864105e-11\n",
      "3  20260119-1730_TERRAMINDxS2                    9.834597e-11\n",
      "4  20260118-2026_TERRAMINDxS2                   -9.260230e-11\n",
      "5  20260118-1020_TERRAMINDxS2                   -9.008875e-11\n",
      "6  20260116-0743_TERRAMINDxS2                   -1.372599e-10\n",
      "7  20260119-0721_TERRAMINDxS2                   -4.266132e-11\n",
      "8  20260117-2359_TERRAMINDxS2                   -1.302630e-10\n",
      "9  20260116-1805_TERRAMINDxS2                   -2.024711e-10\n",
      "\n",
      "Saved updated CSV to: N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\results\\TERRAMIND\\S2\\evaluation_tm_with_uncertainty.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "\n",
    "input_basedir = r\"N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\results\\TERRAMIND\"\n",
    "modality = \"S2\"\n",
    "\n",
    "base = Path(input_basedir) / modality\n",
    "folders = [p for p in base.iterdir() if p.is_dir()]\n",
    "\n",
    "def mean_uncertainty_in_folder(folder_path: Path, suffix: str) -> float:\n",
    "    \"\"\"Weighted mean across ALL valid pixels in ALL tif files matching suffix.\"\"\"\n",
    "    tif_files = list(folder_path.glob(f\"*{suffix}.tif\"))\n",
    "    if not tif_files:\n",
    "        return np.nan\n",
    "\n",
    "    total_sum = 0.0\n",
    "    total_count = 0\n",
    "\n",
    "    for tif_path in tif_files:\n",
    "        with rasterio.open(tif_path) as src:\n",
    "            data = src.read(1, masked=True).astype(np.float64)  # masked array\n",
    "\n",
    "            # data.mask can be scalar False if no mask -> make it array-like\n",
    "            mask = np.zeros(data.shape, dtype=bool) if data.mask is False else data.mask\n",
    "\n",
    "            valid = (~mask) & np.isfinite(data.data)\n",
    "            total_sum += float(data.data[valid].sum())\n",
    "            total_count += int(valid.sum())\n",
    "\n",
    "    return (total_sum / total_count) if total_count > 0 else np.nan\n",
    "\n",
    "\n",
    "# --- compute per-folder mean epistemic uncertainty ---\n",
    "rows = []\n",
    "for folder in folders:\n",
    "    folder_name = folder.name\n",
    "    mean_epi = mean_uncertainty_in_folder(folder, suffix=\"_epistemic\")\n",
    "    rows.append({\"checkpoint_id\": folder_name, \"mean_epistemic_uncertainty_new\": mean_epi})\n",
    "\n",
    "new_df = pd.DataFrame(rows)\n",
    "print(\"Computed per-folder uncertainties:\")\n",
    "print(new_df)\n",
    "\n",
    "# --- load CSV (first .csv in basedir) ---\n",
    "csvs = [p for p in Path(os.path.join(input_basedir, modality)).iterdir() if p.suffix.lower() == \".csv\"]\n",
    "if not csvs:\n",
    "    raise FileNotFoundError(f\"No CSV found in {input_basedir}\")\n",
    "input_csv_path = csvs[0]\n",
    "\n",
    "input_df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# create a key from checkpoint_path to match folder name\n",
    "input_df[\"checkpoint_id\"] = input_df[\"checkpoint_path\"].apply(lambda p: Path(str(p)).stem)\n",
    "\n",
    "# merge + overwrite the target column\n",
    "merged = input_df.merge(new_df, on=\"checkpoint_id\", how=\"left\")\n",
    "\n",
    "# if your CSV column is already called mean_epistemic_uncertainty, overwrite it\n",
    "merged[\"mean_epistemic_uncertainty\"] = merged[\"mean_epistemic_uncertainty_new\"].combine_first(\n",
    "    merged.get(\"mean_epistemic_uncertainty\")\n",
    ")\n",
    "\n",
    "merged = merged.drop(columns=[\"mean_epistemic_uncertainty_new\"])\n",
    "\n",
    "# --- sanity check: which rows didn't match any folder? ---\n",
    "missing = merged[merged[\"mean_epistemic_uncertainty\"].isna()][[\"checkpoint_path\", \"checkpoint_id\"]]\n",
    "if len(missing) > 0:\n",
    "    print(\"\\nWARNING: These rows did not get updated (no matching folder or no tif files):\")\n",
    "    print(missing.head(20))\n",
    "\n",
    "# --- save ---\n",
    "out_path = input_csv_path.with_name(input_csv_path.stem + \"_with_uncertainty.csv\")\n",
    "merged.to_csv(out_path, index=False)\n",
    "print(f\"\\nSaved updated CSV to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c2835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
