{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8a0bb76",
   "metadata": {},
   "source": [
    "## Bayesian ANOVA\n",
    "\n",
    "**Bayesian ANOVA** offers a more flexible and informative alternative to the traditional **frequentist ANOVA**. While classical ANOVA tests whether there are any statistically significant differences between group means, it comes with several limitations:\n",
    "\n",
    "- It provides only a *p-value*, not the size or uncertainty of effects.\n",
    "- It assumes normality, homogeneity of variance, and fixed effects.\n",
    "- It gives no direct probability for hypotheses—just whether the null is rejected or not.\n",
    "\n",
    "In contrast, **Bayesian ANOVA** uses probability distributions to directly model uncertainty and effect sizes. Instead of asking whether group means are different *in general*, it estimates:\n",
    "\n",
    "- The **posterior distribution** of each group mean\n",
    "- The **probability** of differences between groups\n",
    "- The **credible intervals** that reflect uncertainty in estimates\n",
    "\n",
    "Bayesian ANOVA can also naturally handle more complex models (e.g., hierarchical structures, unequal variances) and allows for **model comparison using Bayes Factors**, offering a principled way to weigh evidence for competing hypotheses.\n",
    "\n",
    "In short, **Bayesian ANOVA** goes beyond just testing for significance—it provides a **deeper, probabilistic understanding** of group differences, effect sizes, and model credibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40bb7ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with PyMC version: 5.22.0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from scipy.stats import gaussian_kde, norm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import scipy.stats as st\n",
    "from scipy.stats import gaussian_kde\n",
    "from typing import Sequence, Tuple\n",
    "\n",
    "print(\"Running with PyMC version:\", pm.__version__)\n",
    "\n",
    "# paths \n",
    "# here we have paths to folders where the logging histories of individual training runs are stored.\n",
    "\n",
    "# for Unet and Swin Transformer models trained on MACS, we have two sets of logs:\n",
    "unet_macs_loss_dir = r'N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\logs\\unet_ae_samples'\n",
    "swin_macs_loss_dir = r'N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\logs\\swin_ae_samples'\n",
    "\n",
    "# for Unet and Swin Transformer models trained on PS, we have two sets of logs:\n",
    "unet_ps_loss_dir = r'N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\logs\\unet_ps_samples'\n",
    "swin_ps_loss_dir = r'N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\logs\\swin_ps_samples'\n",
    "\n",
    "# these are the metrics that we want to analyse plus the ones where higher is better.\n",
    "metrics = ['loss', 'accuracy', 'specificity', 'sensitivity', 'IoU', 'f1_score', 'Hausdorff_distance']\n",
    "maximize_metrics = {'specificity', 'sensitivity', 'IoU', 'f1_score'}\n",
    "\n",
    "# and some output directories where we will save the results of our analysis.\n",
    "unet_macs_output_dir = r'N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\logs\\unet_ae_samples'\n",
    "swin_macs_output_dir = r'N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\logs\\swin_ae_samples'\n",
    "\n",
    "# this function reads the metrics from CSV files in a given directory and returns them as a 3D numpy array, with\n",
    "# the shape (number of files, number of epochs, number of metrics). It also returns a lookup dictionary for metric names\n",
    "\n",
    "def read_metrics_as_array(directory, metrics):\n",
    "    \n",
    "    \"\"\"\n",
    "    Reads CSV files from a directory and extracts specified metrics into a 3D numpy array.\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing CSV files.\n",
    "        metrics (list): List of metric names to extract from the CSV files.\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - data_array (np.ndarray): A 3D numpy array of shape (num_files, num_epochs, num_metrics).\n",
    "            - lookup (dict): A dictionary mapping metric names to their indices in the data array.\n",
    "            - files (list): List of file names processed.\n",
    "    \"\"\"\n",
    "\n",
    "    files = sorted([f for f in os.listdir(directory) if f.endswith('.csv')])\n",
    "    data_list = []\n",
    "    \n",
    "    metric_names = []\n",
    "    for metric in metrics:\n",
    "        metric_names.append(metric)\n",
    "        metric_names.append('val_' + metric)\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        file_data = []\n",
    "\n",
    "        for name in metric_names:\n",
    "            if name in df.columns:\n",
    "                file_data.append(df[name].values)\n",
    "            else:\n",
    "                # Fill with NaNs if column is missing\n",
    "                file_data.append(np.full(len(df), np.nan))\n",
    "        \n",
    "        # Transpose so shape is (epochs, metrics)\n",
    "        file_data = np.stack(file_data, axis=1)  # shape: (epochs, num_metrics)\n",
    "        data_list.append(file_data)\n",
    "\n",
    "    # Convert to a 3D array: (files, epochs, metrics)\n",
    "    data_array = np.stack(data_list, axis=0)\n",
    "\n",
    "    # Build lookup dict\n",
    "    lookup = {name: idx for idx, name in enumerate(metric_names)}\n",
    "\n",
    "    return data_array, lookup, files\n",
    "\n",
    "\n",
    "\n",
    "# lets import and plot the losses for Unet and Swin Transformer models trained on MACS and PS datasets.\n",
    "\n",
    "unet_macs, unet_macs_metric_lookup, unet_macs_file_names = read_metrics_as_array(unet_macs_loss_dir, metrics)\n",
    "swin_macs, swin_macs_metric_lookup, swin_macs_file_names = read_metrics_as_array(swin_macs_loss_dir, metrics)\n",
    "unet_ps, unet_ps_metric_lookup, unet_ps_file_names = read_metrics_as_array(unet_ps_loss_dir, metrics)\n",
    "swin_ps, swin_ps_metric_lookup, swin_ps_file_names = read_metrics_as_array(swin_ps_loss_dir, metrics)\n",
    "\n",
    "\n",
    "# next we will define a function to get the metric values across epochs at the point of the lowest val_loss for each file in the data array.\n",
    "# we also define a function to help interpret the Bayes factor values.\n",
    "\n",
    "def get_best_metric(data_array, metric_lookup, metric):\n",
    "    \"\"\"\n",
    "    Get the best metric values across epochs for each file in the data array.\n",
    "    Args:\n",
    "        data_array (np.ndarray): 3D array of shape (files, epochs, metrics).\n",
    "        metric_lookup (dict): Dictionary mapping metric names to their indices.\n",
    "        metric (str): The metric to evaluate.\n",
    "        maximize_metrics (set): Set of metrics that should be maximized.\n",
    "    Returns:\n",
    "        np.ndarray: Array of best metric values for each file.\n",
    "    \"\"\"\n",
    "\n",
    "    best_values = []\n",
    "\n",
    "\n",
    "    for i in range(data_array.shape[0]):\n",
    "\n",
    "        losses = data_array[i, :, metric_lookup['loss']]\n",
    "        values = data_array[i, :, metric_lookup[metric]]\n",
    "\n",
    "\n",
    "        best_epoch = np.nanargmin(losses)\n",
    "        #print(f'best epoch; {best_epoch}')\n",
    "\n",
    "        best_value = values[best_epoch]\n",
    "        #print('best value:', best_value)\n",
    "\n",
    "        best_values.append(best_value)\n",
    "\n",
    "    return np.array(best_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ef00fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_factor_best(probability_vector_best, best_group_idx, num_groups):\n",
    "\n",
    "    posterior_probability = probability_vector_best[best_group_idx]\n",
    "    prior_probability = 1/num_groups # our prior probability assumption is that all groups are equally good\n",
    "\n",
    "    return(posterior_probability/(1-posterior_probability)) / (prior_probability/(1-prior_probability))\n",
    "\n",
    "def pairwise_bayes_factor(mu_draws, i, j, tau):\n",
    "\n",
    "    data = (mu_draws.sel(mu_dim_0=i) - mu_draws.sel(mu_dim_0=j)).values.ravel()\n",
    "    kde = gaussian_kde(data, bw_method=0.3)\n",
    "    posterior_density_at_0 = kde.evaluate(0)[0]\n",
    "    prior_density_at_0 = st.norm.pdf(0, loc=0, scale=np.sqrt(2)*tau)\n",
    "    return posterior_density_at_0/prior_density_at_0\n",
    "\n",
    "def model(df, group_col, value_col, diagnostics=True):\n",
    "\n",
    "    group_code = pd.Categorical(df[group_col]).codes\n",
    "    unique_groups = pd.Categorical(df[group_col]).categories\n",
    "    num_groups = len(unique_groups)\n",
    "\n",
    "    #display(group_code, unique_groups, num_groups)\n",
    "\n",
    "    with pm.Model() as model:\n",
    "\n",
    "        # Hyperpriors, same for all groups\n",
    "        mu_global = pm.Normal('mu_global', 0.5, 0.3) # global mean for all groups\n",
    "        sigma_global = pm.HalfNormal('sigma_global', 0.2) #global std for all groups\n",
    "\n",
    "        # group level prior, start of as all the same, informed by global hyperprior\n",
    "        mu = pm.Normal('mu', mu_global, sigma_global, shape=num_groups) # group mean\n",
    "        sigma = pm. HalfNormal('sigma', 0.2, shape=num_groups) # group std\n",
    "\n",
    "        # likelyhoods\n",
    "        # degrees of freedom\n",
    "        nu = pm.Exponential('nu', 1/30) + 1\n",
    "\n",
    "        pm.StudentT('data', mu=mu[group_code], sigma=sigma[group_code], nu=nu, observed=df[value_col].values) # pic mu and sigma for each group\n",
    "\n",
    "        deltas = {} # store the difference between posterior means here\n",
    "        for i in range(num_groups):\n",
    "            for j in range(i+1, num_groups):\n",
    "                name = f'delta_{i}_{j}'\n",
    "                deltas[(i, j)] = pm.Deterministic(name, mu[i]-mu[j])\n",
    "\n",
    "        idata = pm.sample(draws=4000, chains=4, target_accept=0.9, return_inferencedata=True, progressbar=True)\n",
    "\n",
    "        delta_names = [v.name for v in deltas.values()]\n",
    "\n",
    "        prior_idata = pm.sample_prior_predictive(var_names=delta_names, return_inferencedata=True, random_seed=99)\n",
    "\n",
    "        idata.extend(prior_idata) \n",
    "\n",
    "        idata = pm.sample_posterior_predictive(trace=idata, var_names=[\"data\"], extend_inferencedata=True,)\n",
    "\n",
    "    #print diagnostics R‑hat < 1.01 and ESS > 400 are typically “good”\n",
    "    summary_indv_performance = az.summary(idata, var_names=['mu', 'sigma'])\n",
    "    #display(summary_indv_performance)\n",
    "\n",
    "    summary_diff = az.summary(idata, var_names=delta_names)\n",
    "    #display(summary_diff)\n",
    "\n",
    "    # draw posteriro probability that each group is best (here lowest/highest mu)\n",
    "    mu_draws = idata.posterior['mu'] #--> shape of chain x draw x num_groups\n",
    "    mu_draws_flattened = mu_draws.stack(d=('chain', 'draw'))\n",
    "\n",
    "    #count which group is best per draw\n",
    "    if value_col in ['val_loss', 'loss', 'Hausdorff_distance', 'val_Hausdorff_distance']:\n",
    "        best_count =mu_draws_flattened.argmin(dim='mu_dim_0').values # which group is best per draw, here lowest mu\n",
    "    \n",
    "    else:\n",
    "        best_count= mu_draws_flattened.argmax(dim='mu_dim_0').values # which group is best per draw, here lowest \n",
    "    \n",
    "    prob_best = np.bincount(best_count, minlength=num_groups)/best_count.size\n",
    "    #display(prob_best)\n",
    "\n",
    "    #print(f'\\nProbability that each group has best {value_col}:')\n",
    "    for group_idx, probability in enumerate(prob_best):\n",
    "        #print(f'{unique_groups[group_idx]:20}: {probability:0.3f}')\n",
    "\n",
    "        BF_best = bayes_factor_best(prob_best, group_idx, num_groups)\n",
    "        #print(f\"BF10 that '{unique_groups[group_idx]}' is the overall best:{BF_best:0.2f}\")\n",
    "\n",
    "    rows = []\n",
    "    for (i, j), deltavar in deltas.items():\n",
    "        delta_name = deltavar.name              # e.g. \"delta_0_1\"\n",
    "        bf = az.bayes_factor(idata, var_name=delta_name, ref_val=0)[\"BF10\"]\n",
    "\n",
    "        rows.append({\"comparison\": f\"{unique_groups[i]} vs {unique_groups[j]}\",\"BF10\": bf })\n",
    "    \n",
    "    bf_table = pd.DataFrame(rows)\n",
    "    #display(bf_table)\n",
    "\n",
    "\n",
    "    idx_map = {f\"mu[{i}]\": unique_groups[i] for i in range(num_groups)}\n",
    "    perf_table = (\n",
    "        summary_indv_performance\n",
    "        .loc[[f\"mu[{i}]\" for i in range(num_groups)]]     # keep only μ rows\n",
    "        .rename(index=idx_map)                            # pretty row names\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"group\"})\n",
    "    )\n",
    "\n",
    "    perf_table[\"prob_best\"] = prob_best\n",
    "    perf_table[\"BF10_best\"] = [\n",
    "        bayes_factor_best(prob_best, i, num_groups) for i in range(num_groups)\n",
    "    ]\n",
    "\n",
    "    print('\\nPerformance of Models according to Posterior\\n')\n",
    "    display(perf_table)\n",
    "\n",
    "    import re\n",
    "\n",
    "    def parse_delta(name):\n",
    "        i, j = map(int, re.findall(r\"\\d+\", name))   # delta_2_3 → (2,3)\n",
    "        return f\"{unique_groups[i]} vs {unique_groups[j]}\"\n",
    "\n",
    "    print('\\nDifference in Model Performacne according to Posterior\\n')\n",
    "    diff_table = (\n",
    "        summary_diff\n",
    "        .assign(comparison=lambda df_: df_.index.map(parse_delta))\n",
    "        .reset_index(drop=True)\n",
    "        .merge(bf_table, on=\"comparison\")           # add the BF10 column\n",
    "        .loc[:, [\"comparison\", \"mean\", \"sd\", \"hdi_3%\", \"hdi_97%\", \"BF10\"]]\n",
    "    )\n",
    "\n",
    "    display(diff_table)\n",
    "    \n",
    "    \n",
    "    az.plot_forest(idata, var_names=[\"mu\"], hdi_prob=0.94, combined=True)\n",
    "\n",
    "    #az.plot_ppc(idata, num_pp_samples=100)\n",
    "    #plt.show()\n",
    "\n",
    "    if diagnostics:\n",
    "        import seaborn as sns\n",
    "        sns.set(style=\"whitegrid\")\n",
    "\n",
    "        # ──────────────────────────\n",
    "        # Sampler-diagnostics\n",
    "        # ──────────────────────────\n",
    "        print(\"T\\nRACE PLOTS → confirm chain mixing and stationarity.\")\n",
    "        az.plot_trace(idata, var_names=[\"mu\", \"sigma\", \"mu_global\", \"sigma_global\", \"nu\"], legend=True)\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\nENERGY PLOT → overlay of energy & histogram; look for overlap (no bimodality).\")\n",
    "        az.plot_energy(idata)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\nDIVERGENCE PAIRS → red ×’s would mark where NUTS diverged (should be empty).\")\n",
    "        az.plot_pair(idata, var_names=[\"mu\"], kind=\"kde\", divergences=True,\n",
    "                    coords={\"mu_dim_0\": np.arange(len(unique_groups))})\n",
    "        plt.suptitle(\"Divergences (if any) over μ-space\")\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\nPAIR PLOT → examine correlations or multimodality among group means.\")\n",
    "        az.plot_pair(idata, var_names=[\"mu\"], kind=\"kde\", marginals=True)\n",
    "        plt.suptitle(\"Joint posterior of group means (μ)\"); plt.show()\n",
    "\n",
    "\n",
    "        # ──────────────────────────\n",
    "        # Shrinkage check\n",
    "        # ──────────────────────────\n",
    "        print(\"\\nSHRINKAGE LINES → how far each empirical mean is pulled toward the hierarchy.\")\n",
    "        raw_means = df.groupby(\"group\")[value_col].mean().rename(\"empirical_mean\")\n",
    "        posterior_means = perf_table.set_index(\"group\")[\"mean\"].rename(\"posterior_mean\")\n",
    "        shrink_df = pd.concat([raw_means, posterior_means], axis=1).reset_index()\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        for _, row in shrink_df.iterrows():\n",
    "            plt.plot([0, 1], [row[\"empirical_mean\"], row[\"posterior_mean\"]], marker=\"o\")\n",
    "        plt.xticks([0, 1], [\"empirical\", \"posterior\"])\n",
    "        plt.title(\"Shrinkage of group means toward the hierarchy\")\n",
    "        plt.ylabel(value_col)\n",
    "        for _, row in shrink_df.iterrows():\n",
    "            plt.text(1.02, row[\"posterior_mean\"], row[\"group\"], va=\"center\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "        # ──────────────────────────\n",
    "        # Probability-of-superiority heat-map\n",
    "        # ──────────────────────────\n",
    "        print(\"\\nHEAT-MAP > cell (i,j) = Pr(group i beats group j). 0.5 on diagonal.\")\n",
    "        G = len(unique_groups)\n",
    "        mu_stacked = (idata.posterior[\"mu\"].stack(sample=(\"chain\", \"draw\"))\n",
    "                    .transpose(\"mu_dim_0\", \"sample\").values)\n",
    "\n",
    "        if value_col in ['val_loss', 'loss', 'Hausdorff_distance', 'val_Hausdorff_distance']:\n",
    "\n",
    "            psup = np.array([[0.5 if i==j else (mu_stacked[i] < mu_stacked[j]).mean()\n",
    "                            for j in range(G)] for i in range(G)])\n",
    "            \n",
    "        else:\n",
    "            psup = np.array([[0.5 if i==j else (mu_stacked[i] > mu_stacked[j]).mean()\n",
    "                        for j in range(G)] for i in range(G)])\n",
    "\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(psup, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "                    xticklabels=unique_groups, yticklabels=unique_groups)\n",
    "        plt.title(\"Pr( μ_row < μ_col )  — smaller val_loss is better\"); plt.show()\n",
    "\n",
    "\n",
    "        # ──────────────────────────\n",
    "        # Evidence visualisations\n",
    "        # ──────────────────────────\n",
    "        print(\"\\nBAR (log BF10) → strength of evidence each group is overall best.\")\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        sns.barplot(x=\"group\", y=\"BF10_best\", data=perf_table, color=\"steelblue\")\n",
    "        plt.yscale(\"log\"); plt.ylabel(\"BF10 (log scale)\")\n",
    "        plt.title(\"Evidence each group is the overall best\")\n",
    "        plt.xticks(rotation=45, ha=\"right\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "        print(\"\\nWATERFALL → effect sizes (Δμ) with 95% HDI; crosses zero → weak evidence.\")\n",
    "        sorted_diff = diff_table.sort_values(\"mean\")\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.hlines(y=sorted_diff[\"comparison\"], xmin=sorted_diff[\"hdi_3%\"],\n",
    "                xmax=sorted_diff[\"hdi_97%\"], color=\"gray\")\n",
    "        plt.scatter(sorted_diff[\"mean\"], sorted_diff[\"comparison\"], color=\"red\")\n",
    "        plt.axvline(0, color=\"black\", ls=\"--\")\n",
    "        plt.xlabel(\"Posterior mean difference Δμ\")\n",
    "        plt.title(\"Effect sizes and 95% HDI\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "        print(\"\\nLOG-BF HEAT-MAP → positive = row outranks col; magnitude shows evidence.\")\n",
    "        bf_matrix = pd.DataFrame(0.0, index=unique_groups, columns=unique_groups)\n",
    "        for _, row in diff_table.iterrows():\n",
    "            g1, g2 = row[\"comparison\"].split(\" vs \")\n",
    "            #bf_matrix.loc[g1, g2] = np.log10(row[\"BF10\"])\n",
    "            #bf_matrix.loc[g2, g1] = -np.log10(row[\"BF10\"]) #log\n",
    "            bf_matrix.loc[g1, g2] = row[\"BF10\"] #normal\n",
    "            bf_matrix.loc[g2, g1] = row[\"BF10\"]  # reciprocal\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(bf_matrix, annot=True, fmt=\".1f\", center=0, cmap=\"coolwarm\")\n",
    "        plt.title(\"log₁₀ BF10  (positive → row better)\"); plt.show()\n",
    "\n",
    "    return perf_table, diff_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199ee274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96dd4e00efc64d7784afc52570039b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output(), Output(), Output(), Output(), Output(), Output()), selected_index=0, titles=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tabs = []\n",
    "tab_titles = []\n",
    "\n",
    "#metrics = metrics []\n",
    "#print(metrics)\n",
    "\n",
    "for metric in metrics: \n",
    "    full_metric = 'val_' + metric if not metric.startswith('val_') else metric\n",
    "    out = widgets.Output()\n",
    "\n",
    "    with out:\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "\n",
    "        print(f'Processing: {full_metric} at point of best loss\\n')\n",
    "\n",
    "        unet_macs_best = get_best_metric(unet_macs, unet_macs_metric_lookup, full_metric)\n",
    "        swin_macs_best = get_best_metric(swin_macs, swin_macs_metric_lookup, full_metric)\n",
    "\n",
    "        unet_ps_best = get_best_metric(unet_ps, unet_ps_metric_lookup, full_metric)\n",
    "        swin_ps_best = get_best_metric(swin_ps, swin_ps_metric_lookup, full_metric)\n",
    "\n",
    "        data = pd.concat([\n",
    "                        pd.DataFrame({full_metric: unet_macs_best, 'group': 'U-Net | Aerial'}),\n",
    "                        pd.DataFrame({full_metric: swin_macs_best, 'group': 'Swin U-Net | Aerial'}),\n",
    "                        pd.DataFrame({full_metric: swin_ps_best, 'group': 'Swin U-Net | PS'}),\n",
    "                        pd.DataFrame({full_metric: unet_ps_best, 'group': 'U-Net | PS'})\n",
    "                    ]).reset_index(drop=True)\n",
    "\n",
    "        sns.kdeplot(data, x=full_metric, hue='group')\n",
    "        plt.show()\n",
    "\n",
    "        ANOVA(data, 'group', full_metric, diagnostics=True)\n",
    "\n",
    "    tabs.append(out)\n",
    "    tab_titles.append(full_metric)\n",
    "\n",
    "tab_widget = widgets.Tab(children=tabs)\n",
    "\n",
    "for i, title in enumerate(tab_titles):\n",
    "    tab_widget.set_title(i, title)\n",
    "\n",
    "display(tab_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35481562",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'perf_table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m pred = \u001b[43mperf_table\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      2\u001b[39m obs = mean_by_group[\u001b[33m'\u001b[39m\u001b[33mval_loss_mean\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      4\u001b[39m fit_x = [\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'perf_table' is not defined"
     ]
    }
   ],
   "source": [
    "pred = perf_table['mean']\n",
    "obs = mean_by_group['val_loss_mean']\n",
    "\n",
    "fit_x = [0,1]\n",
    "fit_y =[0,1]\n",
    "\n",
    "display(pred)\n",
    "display(obs)\n",
    "\n",
    "plt.scatter(x=obs, y=pred)\n",
    "plt.plot(fit_x, fit_y)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print('RMSE: ', np.sqrt(mean_squared_error(obs, pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c911114b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plotting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
