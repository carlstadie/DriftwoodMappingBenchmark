{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3270551a",
   "metadata": {},
   "source": [
    "# Bayesian Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f48a38c",
   "metadata": {},
   "source": [
    "This notebook can be used to conduct bayesian ttests and ANOVAS with pymc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eae860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from scipy.stats import gaussian_kde, norm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "print(\"Running with PyMC version:\", pm.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a135b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths \n",
    "# here we have paths to folders where the logging histories of individual training runs are stored.\n",
    "\n",
    "# for Unet and Swin Transformer models trained on MACS, we have two sets of logs:\n",
    "unet_macs_loss_dir = r'N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\logs\\unet_ae_samples'\n",
    "swin_macs_loss_dir = r'N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\logs\\swin_ae_samples'\n",
    "\n",
    "# for Unet and Swin Transformer models trained on PS, we have two sets of logs:\n",
    "unet_ps_loss_dir = r'N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\logs\\unet_ps_samples'\n",
    "swin_ps_loss_dir = r'N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\logs\\swin_ps_samples'\n",
    "\n",
    "# these are the metrics that we want to analyse plus the ones where higher is better.\n",
    "metrics = ['loss', 'specificity', 'sensitivity', 'IoU', 'f1_score', 'Hausdorff_distance']\n",
    "maximize_metrics = {'specificity', 'sensitivity', 'IoU', 'f1_score'}\n",
    "\n",
    "# and some output directories where we will save the results of our analysis.\n",
    "unet_macs_output_dir = r'N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\logs\\unet_ae_samples'\n",
    "swin_macs_output_dir = r'N:\\isipd\\projects\\p_planetdw\\data\\methods_test\\logs\\swin_ae_samples'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ff3439",
   "metadata": {},
   "source": [
    "## Read and plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b3e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function reads the metrics from CSV files in a given directory and returns them as a 3D numpy array, with\n",
    "# the shape (number of files, number of epochs, number of metrics). It also returns a lookup dictionary for metric names\n",
    "\n",
    "def read_metrics_as_array(directory, metrics):\n",
    "    \n",
    "    \"\"\"\n",
    "    Reads CSV files from a directory and extracts specified metrics into a 3D numpy array.\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing CSV files.\n",
    "        metrics (list): List of metric names to extract from the CSV files.\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - data_array (np.ndarray): A 3D numpy array of shape (num_files, num_epochs, num_metrics).\n",
    "            - lookup (dict): A dictionary mapping metric names to their indices in the data array.\n",
    "            - files (list): List of file names processed.\n",
    "    \"\"\"\n",
    "\n",
    "    files = sorted([f for f in os.listdir(directory) if f.endswith('.csv')])\n",
    "    data_list = []\n",
    "    \n",
    "    metric_names = []\n",
    "    for metric in metrics:\n",
    "        metric_names.append(metric)\n",
    "        metric_names.append('val_' + metric)\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        file_data = []\n",
    "\n",
    "        for name in metric_names:\n",
    "            if name in df.columns:\n",
    "                file_data.append(df[name].values)\n",
    "            else:\n",
    "                # Fill with NaNs if column is missing\n",
    "                file_data.append(np.full(len(df), np.nan))\n",
    "        \n",
    "        # Transpose so shape is (epochs, metrics)\n",
    "        file_data = np.stack(file_data, axis=1)  # shape: (epochs, num_metrics)\n",
    "        data_list.append(file_data)\n",
    "\n",
    "    # Convert to a 3D array: (files, epochs, metrics)\n",
    "    data_array = np.stack(data_list, axis=0)\n",
    "\n",
    "    # Build lookup dict\n",
    "    lookup = {name: idx for idx, name in enumerate(metric_names)}\n",
    "\n",
    "    return data_array, lookup, files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe75c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function plots the val and train metric from the array of losses.\n",
    "\n",
    "def plot_losses(loss_array, metrics, metric_lookup, output_dir, show_plot=False):\n",
    "    \"\"\"\n",
    "    Plots training and validation metrics from a 3D numpy array of losses.\n",
    "    Args:\n",
    "        loss_array (np.ndarray): 3D numpy array of shape (num_files, num_epochs, num_metrics).\n",
    "        metrics (list): List of metric names to plot.\n",
    "        metric_lookup (dict): Dictionary mapping metric names to their indices in the loss_array.\n",
    "        output_dir (str): Directory to save the plot.\n",
    "        show_plot (bool): Whether to display the plot interactively.\n",
    "    \"\"\"\n",
    "\n",
    "    epochs = loss_array.shape[1]\n",
    "    num_metrics = len(metrics)\n",
    "\n",
    "    plt.figure(figsize=(20, 2.5))  # Square layout\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(1, num_metrics, i + 1)\n",
    "\n",
    "        # Training metric\n",
    "        for j in range(loss_array.shape[0]):\n",
    "            plt.plot(range(epochs), loss_array[j, :, metric_lookup[metric]], color='lightblue', linewidth=1)\n",
    "\n",
    "        train_mean = np.nanmean(loss_array[:, :, metric_lookup[metric]], axis=0)\n",
    "        \n",
    "        # Validation metric\n",
    "        val_metric = 'val_' + metric\n",
    "        if val_metric in metric_lookup:\n",
    "            for j in range(loss_array.shape[0]):\n",
    "                plt.plot(range(epochs), loss_array[j, :, metric_lookup[val_metric]], color='peachpuff', linewidth=1)\n",
    "\n",
    "            val_mean = np.nanmean(loss_array[:, :, metric_lookup[val_metric]], axis=0)\n",
    "            plt.plot(range(epochs), train_mean, color='tab:blue', label=f'train', linewidth=2)\n",
    "            plt.plot(range(epochs), val_mean, color='tab:orange', label=f'val', linewidth=2)\n",
    "\n",
    "        plt.title(metric)\n",
    "        plt.ylim(0,1)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel(metric)\n",
    "        #plt.legend(loc='upper right')\n",
    "        plt.grid(True)\n",
    "        plt.gca().set_aspect('auto')  # Square plot per metric (approx)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    output_path = os.path.join(output_dir, 'losses_plot.png')\n",
    "    plt.savefig(output_path)\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda0b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import and plot the losses for Unet and Swin Transformer models trained on MACS and PS datasets.\n",
    "\n",
    "unet_macs, unet_macs_metric_lookup, unet_macs_file_names = read_metrics_as_array(unet_macs_loss_dir, metrics)\n",
    "swin_macs, swin_macs_metric_lookup, swin_macs_file_names = read_metrics_as_array(swin_macs_loss_dir, metrics)\n",
    "unet_ps, unet_ps_metric_lookup, unet_ps_file_names = read_metrics_as_array(unet_ps_loss_dir, metrics)\n",
    "swin_ps, swin_ps_metric_lookup, swin_ps_file_names = read_metrics_as_array(swin_ps_loss_dir, metrics)\n",
    "\n",
    "plot_losses(unet_macs, metrics, unet_macs_metric_lookup, unet_macs_output_dir, show_plot=True)\n",
    "plot_losses(swin_macs, metrics, swin_macs_metric_lookup, swin_macs_output_dir, show_plot=True)\n",
    "plot_losses(unet_ps, metrics, unet_ps_metric_lookup, unet_ps_loss_dir, show_plot=True)\n",
    "plot_losses(swin_ps, metrics, swin_ps_metric_lookup, swin_ps_loss_dir, show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb4ca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we will define a function to get the metric values across epochs at the point of the lowest val_loss for each file in the data array.\n",
    "# we also define a function to help interpret the Bayes factor values.\n",
    "\n",
    "def get_best_metric(data_array, metric_lookup, metric):\n",
    "    \"\"\"\n",
    "    Get the best metric values across epochs for each file in the data array.\n",
    "    Args:\n",
    "        data_array (np.ndarray): 3D array of shape (files, epochs, metrics).\n",
    "        metric_lookup (dict): Dictionary mapping metric names to their indices.\n",
    "        metric (str): The metric to evaluate.\n",
    "        maximize_metrics (set): Set of metrics that should be maximized.\n",
    "    Returns:\n",
    "        np.ndarray: Array of best metric values for each file.\n",
    "    \"\"\"\n",
    "\n",
    "    best_values = []\n",
    "\n",
    "\n",
    "    for i in range(data_array.shape[0]):\n",
    "\n",
    "        losses = data_array[i, :, metric_lookup['loss']]\n",
    "        values = data_array[i, :, metric_lookup[metric]]\n",
    "\n",
    "\n",
    "        best_epoch = np.nanargmin(losses)\n",
    "        #print(f'best epoch; {best_epoch}')\n",
    "\n",
    "        best_value = values[best_epoch]\n",
    "        #print('best value:', best_value)\n",
    "\n",
    "        best_values.append(best_value)\n",
    "\n",
    "    return np.array(best_values)\n",
    "\n",
    "def interpret_bayes_factor(bf):\n",
    "    \"\"\"Return Jeffreys-style verbal label for a Bayes factor > 1.\"\"\"\n",
    "    if bf < 3:\n",
    "        return \"anecdotal\"\n",
    "    elif bf < 10:\n",
    "        return \"moderate\"\n",
    "    elif bf < 30:\n",
    "        return \"strong\"\n",
    "    elif bf < 100:\n",
    "        return \"very strong\"\n",
    "    else:\n",
    "        return \"extreme\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acba760",
   "metadata": {},
   "source": [
    "## BEST Bayesian T-test\n",
    "\n",
    "The **BEST (Bayesian Estimation Supersedes the t-Test)** approach offers a modern, more informative alternative to the traditional **t-test**. While the t-test provides a p-value to assess whether two group means are significantly different, it relies heavily on assumptions like:\n",
    "\n",
    "- Normality of data\n",
    "- Equal variances\n",
    "- A fixed significance threshold (e.g., *p* < 0.05)\n",
    "\n",
    "Moreover, the t-test doesn't convey the size or uncertainty of the effect in an intuitive way.\n",
    "\n",
    "In contrast, **BEST** uses Bayesian methods to estimate the full **posterior distribution** of the group means and their difference. This approach provides a richer understanding, including:\n",
    "\n",
    "- How large the difference might be\n",
    "- How uncertain we are about that difference\n",
    "- The probability that one group is greater than the other\n",
    "\n",
    "BEST is also more robust to common issues like unequal variances and outliers.\n",
    "\n",
    "In short, **BEST supersedes the t-test** by delivering **more nuanced, probabilistic insights** into group comparisons, rather than a single binary decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d84d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BEST(combined_array, group_one, group_two, metric, group_one_label='unet', group_two_label='swin', plot=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Perform Bayesian estimation of the difference in means and standard deviations\n",
    "    between two groups using Student's t-distribution (Kruschke 2005).\n",
    "    Args:\n",
    "        combined_array (pd.DataFrame): DataFrame containing the metric values and group labels.\n",
    "        group_one (np.ndarray): values of group 1.\n",
    "        group_two (np.ndarray): values of group 2.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    minimal_metrcis = ['val_loss', 'loss', 'val_Hausdorff_distance', 'Hausdorff_distance']\n",
    "\n",
    "    metric_values = combined_array[metric].values\n",
    "\n",
    "    mu_m = metric_values.mean()\n",
    "    mu_s = metric_values.std()*2\n",
    "\n",
    "    sigma_low = 10**-1\n",
    "    sigma_high = 10\n",
    "\n",
    "    # define the model. change the priors as needed. \n",
    "\n",
    "    with pm.Model() as model:\n",
    "\n",
    "        # Priors for group means (share common hyperpriors mu_m, mu_s)\n",
    "        # We place Normal priors on each group’s mean, centered at a common location (mu_m) with shared uncertainty (mu_s) comming from the data (weakly informed).\n",
    "        group1_mean = pm.Normal(f'{group_one_label}_mean', mu=mu_m, sigma=mu_s)\n",
    "        group2_mean = pm.Normal(f'{group_two_label}_mean', mu=mu_m, sigma=mu_s)\n",
    "\n",
    "        # Priors for group standard deviations (allowing unequal variances)\n",
    "        # Each group’s scale is given an independent Uniform prior between sigma_low and sigma_high, allowing heterogeneity (previously defined --> uninformend as not from the data).\n",
    "        group1_std = pm.Uniform(f'{group_one_label}_std', lower=sigma_low, upper=sigma_high)\n",
    "        group2_std = pm.Uniform(f'{group_two_label}_std', lower=sigma_low, upper=sigma_high)\n",
    "\n",
    "        # Degrees of freedom for Student-t (robust to outliers)\n",
    "        #To model heavy tails and robustness to outliers, we use a Student-t likelihood. We put an Exponential prior on nu − 1, shift it to nu, and also expose its log₁₀ for diagnostics.\n",
    "        nu_minus_one = pm.Exponential('nu_minus_one', lam=1/29)\n",
    "        nu = pm.Deterministic('nu', nu_minus_one + 1)\n",
    "        nu_log10 = pm.Deterministic('nu_log10', np.log10(nu))\n",
    "\n",
    "        # Convert std to precision for the Student-t likelihood\n",
    "        # Student-t in PyMC uses precision (λ = σ⁻²), so we square-invert the std priors.\n",
    "        lambda_group1 = group1_std**-2\n",
    "        lambda_group2 = group2_std**-2\n",
    "\n",
    "        # Observation models for each group\n",
    "        # Each group’s data are modeled as Student-t draws with their own mean, precision, and shared ν.\n",
    "        group_one_obs = pm.StudentT(f'{group_one_label}_obs', mu=group1_mean, lam=lambda_group1, nu=nu, observed=group_one)\n",
    "        group_two_obs = pm.StudentT(f'{group_two_label}_obs', mu=group2_mean, lam=lambda_group2, nu=nu, observed=group_two)\n",
    "\n",
    "        # Deterministic comparisons for interpretation\n",
    "        # We compute the difference of means, difference of stds, and Cohen’s d–style effect size for easy interpretation of the posterior.\n",
    "        diff_of_means = pm.Deterministic('diff_of_means', group1_mean - group2_mean)\n",
    "        diff_of_stds = pm.Deterministic('diff_of_stds', group1_std - group2_std)\n",
    "        effect_size = pm.Deterministic('effect_size', diff_of_means / np.sqrt((group1_std**2 + group2_std**2) / 2))\n",
    "\n",
    "        # Posterior sampling\n",
    "        idata = pm.sample(tune=1000, draws=2000, chains=4, target_accept=0.95, return_inferencedata=True)\n",
    "\n",
    "    if plot:\n",
    "        # Plotting the posterior distributions and summaries\n",
    "        print('\\n---- Posterior for the means and stds ----')\n",
    "\n",
    "        az.plot_posterior(idata, var_names=[f'{group_one_label}_mean', f'{group_two_label}_mean', f'{group_one_label}_std', f'{group_two_label}_std', 'nu_log10', 'nu'])\n",
    "        plt.show()\n",
    "\n",
    "        print('\\n---- Posterior for the differences and effect size ----')\n",
    "\n",
    "        az.plot_posterior(idata, var_names=['diff_of_means', 'diff_of_stds', 'effect_size'], ref_val=0)\n",
    "        plt.show()\n",
    "\n",
    "        print('\\n---- Forests for means, stds, and nu ----')\n",
    "        \n",
    "        az.plot_forest(idata, var_names=[f'{group_one_label}_mean', f'{group_two_label}_mean'])\n",
    "        plt.show()\n",
    "\n",
    "        print('\\n---- Forests for stds and nu ----')\n",
    "\n",
    "        az.plot_forest(idata, var_names=[f'{group_one_label}_std', f'{group_two_label}_std', 'nu'])\n",
    "        plt.show()\n",
    "\n",
    "    print('\\n---- Model summary ----')\n",
    "\n",
    "    summary = az.summary(idata, var_names=[f'{group_one_label}_mean', f'{group_two_label}_mean','diff_of_means', 'diff_of_stds', 'effect_size'])\n",
    "    print(summary)\n",
    "\n",
    "    print('\\n---- Savage-Dickey Bayes Factor ----\\n')\n",
    "\n",
    "    \n",
    "    # Posterior density at δ = 0  (KDE is still appropriate here)\n",
    "    diff_samples = idata.posterior['diff_of_means'].values.flatten()\n",
    "    posterior_kde            = gaussian_kde(diff_samples)\n",
    "    posterior_density_at_zero = posterior_kde.evaluate(0)[0]\n",
    "\n",
    "    # Analytical prior density at δ = 0  (δ ~ Normal(0, √2·mu_s))\n",
    "    prior_sd_diff            = np.sqrt(2) * mu_s\n",
    "    prior_density_at_zero    = norm.pdf(0, loc=0, scale=prior_sd_diff)\n",
    "\n",
    "    # Bayes factors\n",
    "    BF_01 = posterior_density_at_zero / prior_density_at_zero   # H₀ over H₁\n",
    "    BF_10 = 1 / BF_01                                           # H₁ over H₀\n",
    "\n",
    "    metric_is_lower_better = metric  in minimal_metrcis\n",
    "    mean_diff = diff_samples.mean()   # μ_unet − μ_swin\n",
    "\n",
    "    if BF_10 > 1:            # data support the alternative\n",
    "        # who wins, given the metric direction?\n",
    "        if (not metric_is_lower_better and mean_diff > 0) or \\\n",
    "        (    metric_is_lower_better and mean_diff < 0):\n",
    "            winner, loser = group_one_label, group_two_label\n",
    "        else:\n",
    "            winner, loser = group_two_label, group_one_label\n",
    "\n",
    "        label = interpret_bayes_factor(BF_10)\n",
    "        print(f\"p(δ=0)        : {prior_density_at_zero:.4g}\")\n",
    "        print(f\"p(δ=0 | data) : {posterior_density_at_zero:.4g}\")\n",
    "        print(f\"Evidence for {winner} outperforming {loser}: \"\n",
    "            f\"BF₁₀ = {BF_10:.4g}  ({label})\\n\")\n",
    "\n",
    "    else:                     # data support the null\n",
    "        label = interpret_bayes_factor(BF_01)\n",
    "        print(f\"p(δ=0)        : {prior_density_at_zero:.4g}\")\n",
    "        print(f\"p(δ=0 | data) : {posterior_density_at_zero:.4g}\")\n",
    "        print(f\"Evidence for no difference (H₀): \"\n",
    "            f\"BF₀₁ = {BF_01:.4g}  ({label})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e3a630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we call the BEST function to compare Unet and Swin Transformer models trained on MACS and PS datasets and visualise them as tabs\n",
    "\n",
    "tabs     = []      # the Output widgets (one per metric)\n",
    "tab_titles = []    # used to label the tab headers\n",
    "\n",
    "for metric in metrics:\n",
    "\n",
    "    full_metric = 'val_' + metric if not metric.startswith('val_') else metric\n",
    "    out = widgets.Output()          # each metric gets its own Output “sandbox”\n",
    "\n",
    "    with out:                       # everything inside goes only to this tab\n",
    "        clear_output(wait=True)     # keeps the tab clean on reruns\n",
    "\n",
    "        print(f\"### Processing metric: {full_metric}\\n\")\n",
    "\n",
    "        unet_macs_best = get_best_metric(unet_macs, unet_macs_metric_lookup, full_metric)\n",
    "        swin_macs_best = get_best_metric(swin_macs, swin_macs_metric_lookup, full_metric)\n",
    "\n",
    "        #plt.boxplot([unet_best, swin_best], labels=['UNet', 'Swin Transformer'], showfliers=True)\n",
    "\n",
    "\n",
    "\n",
    "        BEST(\n",
    "            pd.concat([\n",
    "                pd.DataFrame({full_metric: unet_macs_best, 'group': 'U-Net | Aerial'}),\n",
    "                pd.DataFrame({full_metric: swin_macs_best, 'group': 'Swin U-Net | Aerial'}),\n",
    "            ]).reset_index(drop=True),\n",
    "            unet_macs_best, swin_macs_best, full_metric,\n",
    "            plot=True                                    \n",
    "        )\n",
    "\n",
    "    # keep references so we can build the Tab afterwards\n",
    "    tabs.append(out)\n",
    "    tab_titles.append(full_metric)\n",
    "\n",
    "tab_widget = widgets.Tab(children=tabs)\n",
    "\n",
    "for i, title in enumerate(tab_titles):\n",
    "    tab_widget.set_title(i, title)   # label each tab\n",
    "\n",
    "display(tab_widget)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d66d496",
   "metadata": {},
   "source": [
    "## Bayesian ANOVA\n",
    "\n",
    "**Bayesian ANOVA** offers a more flexible and informative alternative to the traditional **frequentist ANOVA**. While classical ANOVA tests whether there are any statistically significant differences between group means, it comes with several limitations:\n",
    "\n",
    "- It provides only a *p-value*, not the size or uncertainty of effects.\n",
    "- It assumes normality, homogeneity of variance, and fixed effects.\n",
    "- It gives no direct probability for hypotheses—just whether the null is rejected or not.\n",
    "\n",
    "In contrast, **Bayesian ANOVA** uses probability distributions to directly model uncertainty and effect sizes. Instead of asking whether group means are different *in general*, it estimates:\n",
    "\n",
    "- The **posterior distribution** of each group mean\n",
    "- The **probability** of differences between groups\n",
    "- The **credible intervals** that reflect uncertainty in estimates\n",
    "\n",
    "Bayesian ANOVA can also naturally handle more complex models (e.g., hierarchical structures, unequal variances) and allows for **model comparison using Bayes Factors**, offering a principled way to weigh evidence for competing hypotheses.\n",
    "\n",
    "In short, **Bayesian ANOVA** goes beyond just testing for significance—it provides a **deeper, probabilistic understanding** of group differences, effect sizes, and model credibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e529cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bayesian_anova(df, metric, lower_is_better=False, robust=True, hierarchical=True, tune=1000, draws=2000, chains=4, target_accept=0.95):\n",
    "    \"\"\"\n",
    "    Fit a Bayesian ANOVA model with options for partial pooling and heavy tails.\n",
    "\n",
    "    In this function, we:\n",
    "      1. Extract the outcome and group labels.\n",
    "      2. Choose weakly informative priors based on the data.\n",
    "      3. Optionally apply partial pooling across groups.\n",
    "      4. Optionally use a Student-T likelihood to guard against outliers.\n",
    "      5. Return posterior samples in an ArviZ InferenceData, including log_likelihood.\n",
    "    \"\"\"\n",
    "    # 1) Prepare data\n",
    "    y = df[metric].values\n",
    "    groups = df[\"group\"].astype(\"category\")\n",
    "    g_idx = groups.cat.codes.values  # convert categories into 0…K-1\n",
    "    K = int(g_idx.max() + 1)\n",
    "\n",
    "    # 2) Set priors based on observed data scale\n",
    "    mu_m, mu_s = y.mean(), y.std() * 2\n",
    "    sigma_low, sigma_high = 1e-1, 10  # bound sigma to [0.1, 10]\n",
    "\n",
    "    with pm.Model() as model:\n",
    "        # --- Priors on group means ---------------------------------------\n",
    "        if hierarchical:\n",
    "            # we assume group means share a global distribution\n",
    "            mu_grand = pm.Normal(\"mu_grand\", mu=mu_m, sigma=mu_s)\n",
    "            tau = pm.HalfNormal(\"tau\", sigma=mu_s)\n",
    "            mu = pm.Normal(\"mu\", mu=mu_grand, sigma=tau, shape=K)\n",
    "        else:\n",
    "            # independent priors for each group mean\n",
    "            mu = pm.Normal(\"mu\", mu=mu_m, sigma=mu_s, shape=K)\n",
    "\n",
    "        # --- Priors on group standard deviations -------------------------\n",
    "        sigma = pm.Uniform(\"sigma\", lower=sigma_low, upper=sigma_high, shape=K)\n",
    "\n",
    "        # --- Likelihood: Student-T for robustness -----------------------\n",
    "        nu = (pm.Exponential(\"nu_minus_1\", 1/29) + 1) if robust else np.inf\n",
    "        pm.StudentT(\"obs\", mu=mu[g_idx], sigma=sigma[g_idx], nu=nu, observed=y)\n",
    "\n",
    "        # 3) Sample from the posterior, including log_likelihood for LOO/WAIC\n",
    "        idata = pm.sample(\n",
    "            tune=tune,\n",
    "            draws=draws,\n",
    "            chains=chains,\n",
    "            target_accept=target_accept,\n",
    "            return_inferencedata=True,\n",
    "            idata_kwargs={\"log_likelihood\": True}\n",
    "        )\n",
    "\n",
    "    # 4) Store metadata for downstream utilities\n",
    "    idata.attrs[\"metric\"] = metric\n",
    "    idata.attrs[\"lower_is_better\"] = lower_is_better\n",
    "    idata.attrs[\"groups\"] = groups.cat.categories.tolist()\n",
    "    return idata\n",
    "\n",
    "\n",
    "def fit_bayesian_null(df, metric, robust=True, tune=1000, draws=2000, chains=4, target_accept=0.95):\n",
    "    \"\"\"\n",
    "    Fit a Bayesian null model where all data share one common mean.\n",
    "    We use this to compute Bayes Factors against the alternative.\n",
    "    \"\"\"\n",
    "    y = df[metric].values\n",
    "    mu_m, mu_s = y.mean(), y.std() * 2\n",
    "    sigma_low, sigma_high = 1e-1, 10\n",
    "\n",
    "    with pm.Model() as null_model:\n",
    "        mu = pm.Normal(\"mu\", mu=mu_m, sigma=mu_s)  # single mean\n",
    "        sigma = pm.Uniform(\"sigma\", lower=sigma_low, upper=sigma_high)\n",
    "        nu = (pm.Exponential(\"nu_minus_1\", 1/29) + 1) if robust else np.inf\n",
    "        pm.StudentT(\"obs\", mu=mu, sigma=sigma, nu=nu, observed=y)\n",
    "\n",
    "        # include log_likelihood for model comparison\n",
    "        idata_null = pm.sample(\n",
    "            tune=tune,\n",
    "            draws=draws,\n",
    "            chains=chains,\n",
    "            target_accept=target_accept,\n",
    "            return_inferencedata=True,\n",
    "            idata_kwargs={\"log_likelihood\": True}\n",
    "        )\n",
    "    return idata_null\n",
    "\n",
    "\n",
    "def compute_bayes_factor(idata_alt, idata_null):\n",
    "    \"\"\"\n",
    "    Estimate Bayes Factor BF_10 = p(data | alt) / p(data | null).\n",
    "    We first try LOO elpd difference. If log_likelihood is missing, we fallback to WAIC.\n",
    "    BF_10 ≈ exp((elpd_alt - elpd_null) / 2).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Compute LOO and extract expected log predictive density\n",
    "        loo_alt = az.loo(idata_alt, pointwise=False)\n",
    "        loo_null = az.loo(idata_null, pointwise=False)\n",
    "        # New ArviZ returns elpd_loo attribute\n",
    "        delta = loo_alt.elpd_loo - loo_null.elpd_loo\n",
    "    except (TypeError, ValueError, AttributeError):\n",
    "        # Fallback to WAIC if LOO fails or missing attributes\n",
    "        waic_alt = az.waic(idata_alt, pointwise=False)\n",
    "        waic_null = az.waic(idata_null, pointwise=False)\n",
    "        # WAICData has elpd_waic attribute\n",
    "        delta = waic_alt.elpd_waic - waic_null.elpd_waic\n",
    "    # Convert elpd difference to approximate Bayes Factor\n",
    "    bf_10 = np.exp(delta / 2)\n",
    "    return bf_10\n",
    "\n",
    "\n",
    "def prob_each_is_best(idata):\n",
    "    \"\"\"\n",
    "    Compute posterior probability that each group is \"best\".\n",
    "\n",
    "    We define \"best\" as having the highest (or lowest) mean across draws.\n",
    "    \"\"\"\n",
    "    metric_is_lower = idata.attrs[\"lower_is_better\"]\n",
    "    # Stack chains and draws into one dimension\n",
    "    means = idata.posterior[\"mu\"].stack(sample=(\"chain\", \"draw\")).values\n",
    "    # Identify index of best mean in each draw\n",
    "    best_idx = means.argmin(axis=0) if metric_is_lower else means.argmax(axis=0)\n",
    "    K = means.shape[0]\n",
    "    # Compute frequency each group is best\n",
    "    p = np.bincount(best_idx, minlength=K) / best_idx.size\n",
    "    return pd.Series(p, index=idata.attrs[\"groups\"], name=\"p(best)\")\n",
    "\n",
    "\n",
    "def pairwise_contrasts(idata, rope=None):\n",
    "    \"\"\"\n",
    "    Summarize pairwise group differences with HDI and probability.\n",
    "\n",
    "    For each pair A vs. B, we compute:\n",
    "      - mean_diff = E[mu_A - mu_B]\n",
    "      - 95% HDI of that difference\n",
    "      - p_A_gt_B = posterior probability that mu_A > mu_B\n",
    "      - optional p_in_rope for Region Of Practical Equivalence\n",
    "    \"\"\"\n",
    "    # Extract posterior means across draws\n",
    "    means = idata.posterior[\"mu\"].stack(sample=(\"chain\", \"draw\")).values\n",
    "    K, _ = means.shape\n",
    "    names = idata.attrs[\"groups\"]\n",
    "    rows = []\n",
    "\n",
    "    # Loop over each unique pair\n",
    "    for i in range(K - 1):\n",
    "        for j in range(i + 1, K):\n",
    "            diff = means[i] - means[j]\n",
    "            # Compute 95% HDI bounds\n",
    "            hdi_low, hdi_high = np.quantile(diff, [0.025, 0.975])\n",
    "            # Probability that A > B in posterior draws\n",
    "            prob_gt0 = (diff > 0).mean()\n",
    "            row = dict(\n",
    "                A=names[i],\n",
    "                B=names[j],\n",
    "                mean_diff=diff.mean(),\n",
    "                hdi_low=hdi_low,\n",
    "                hdi_high=hdi_high,\n",
    "                p_A_gt_B=prob_gt0\n",
    "            )\n",
    "            if rope is not None:\n",
    "                # Fraction of draws within the ROPE\n",
    "                row[\"p_in_rope\"] = ((rope[0] < diff) & (diff < rope[1])).mean()\n",
    "            rows.append(row)\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55568693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build one tab per metric to explore group performance and run our ANOVA.\n",
    "\n",
    "# Define your data sources: mapping group name to (data_array, metric_lookup)\n",
    "group_data = {\n",
    "    \"U-Net | Aerial\": (unet_macs, unet_macs_metric_lookup),\n",
    "    \"U-Net | PS\": (unet_ps, unet_ps_metric_lookup),\n",
    "    \"Swin U-Net | Aerial\": (swin_macs, swin_macs_metric_lookup),\n",
    "    \"Swin U-Net | PS\": (swin_ps, swin_ps_metric_lookup),\n",
    "    # add more groups as needed\n",
    "}\n",
    "\n",
    "# Metrics where lower is better (e.g., loss, distance)\n",
    "minimal_metrics = {\"val_loss\", \"loss\", \"val_Hausdorff_distance\", \"Hausdorff_distance\"}\n",
    "\n",
    "# Helper to extract best-per-run for a given metric\n",
    "\n",
    "def best_per_group(metric_name):\n",
    "    \"\"\"Return dict of {group: best values array} for each run.\"\"\"\n",
    "    out = {}\n",
    "    for name, (arr, lookup) in group_data.items():\n",
    "        # we assume get_best_metric is defined elsewhere\n",
    "        out[name] = get_best_metric(arr, lookup, metric_name)\n",
    "    return out\n",
    "\n",
    "# Build interactive tabs for each metric\n",
    "metrics = [\"loss\", \"accuracy\", \"Hausdorff_distance\"]  # customize your list\n",
    "tabs, titles = [], []\n",
    "for metric in metrics:\n",
    "    full_metric = f\"val_{metric}\"\n",
    "    box = widgets.Output()\n",
    "    with box:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"### Processing metric: {full_metric}\\n\")\n",
    "\n",
    "        # 1) Box-plot of best epoch values\n",
    "        best_vals = best_per_group(full_metric)\n",
    "        plt.boxplot(best_vals.values(), labels=best_vals.keys(), showfliers=True)\n",
    "        plt.title(full_metric)\n",
    "        plt.show()\n",
    "\n",
    "        # 2) Prepare DataFrame for analysis\n",
    "        df = pd.concat([\n",
    "            pd.DataFrame({full_metric: v, \"group\": k})\n",
    "            for k, v in best_vals.items()\n",
    "        ], ignore_index=True)\n",
    "        display(df)\n",
    "\n",
    "        # 3) Fit models and compare\n",
    "        idata_alt = fit_bayesian_anova(\n",
    "            df, full_metric,\n",
    "            lower_is_better=full_metric in minimal_metrics,\n",
    "            hierarchical=True, robust=True\n",
    "        )\n",
    "        idata_null = fit_bayesian_null(df, full_metric, robust=True)\n",
    "\n",
    "        # 4) Summaries\n",
    "        print(\"Posterior probability each group is the best:\")\n",
    "        display(prob_each_is_best(idata_alt).sort_values(ascending=False))\n",
    "\n",
    "        print(\"\\nPairwise contrasts (95% HDI and p):\")\n",
    "        display(pairwise_contrasts(idata_alt, rope=[-0.005, 0.005]))\n",
    "\n",
    "        bf = compute_bayes_factor(idata_alt, idata_null)\n",
    "        print(f\"\\nApproximate Bayes Factor BF_10: {bf:.2f}\")\n",
    "\n",
    "        # 5) Optional plots\n",
    "        az.plot_forest(idata_alt, var_names=\"mu\")\n",
    "        plt.title(f\"{full_metric} – group means with 95% HDI\")\n",
    "        plt.show()\n",
    "\n",
    "    tabs.append(box)\n",
    "    titles.append(full_metric)\n",
    "\n",
    "# Assemble and display the Tab widget\n",
    "tab_widget = widgets.Tab(children=tabs)\n",
    "for i, t in enumerate(titles):\n",
    "    tab_widget.set_title(i, t)\n",
    "display(tab_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4233d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plotting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
